<html lang="zh-CN">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0">
    <title>Warsonco客服机器人</title>
    <script src="https://cdn.jsdelivr.net/pyodide/v0.28.2/full/pyodide.js"></script>
    <script src="config.js"></script>
    <style>
        /* Base styles optimized for mobile */
        * {
            box-sizing: border-box;
            -webkit-tap-highlight-color: transparent;
        }
        body {
            font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, Oxygen, Ubuntu, sans-serif;
            margin: 0;
            padding: 16px;
            background: #f5f5f5;
            color: #333;
            line-height: 1.6;
            touch-action: manipulation;
            -webkit-font-smoothing: antialiased;
        }
        .container {
            max-width: 100%;
            margin: 0 auto;
        }
        h2 {
            text-align: center;
            color: #2c3e50;
            margin-bottom: 20px;
            font-size: 1.5rem;
        }

        /* Input and button styles optimized for touch */
        .input-group {
            display: flex;
            gap: 10px;
            margin-bottom: 20px;
        }
        #inputBox {
            flex: 1;
            padding: 14px 16px;
            font-size: 16px;
            border-radius: 10px;
            border: 1px solid #ddd;
            background: white;
            min-height: 50px;
            -webkit-appearance: none;
        }
        button {
            padding: 14px 20px;
            font-size: 16px;
            border-radius: 10px;
            border: none;
            background: #3498db;
            color: white;
            cursor: pointer;
            min-width: 80px;
            font-weight: 600;
            touch-action: manipulation;
        }
        button:active {
            background: #2980b9;
            transform: scale(0.98);
        }
        button:disabled {
            background: #95a5a6;
            cursor: not-allowed;
        }

        /* Output box */
        #outputBox {
            width: 100%;
            height: 60vh; /* 使用视口高度而不是最小高度 */
            max-height: 60vh; /* 限制最大高度 */
            padding: 16px;
            font-size: 15px;
            border-radius: 10px;
            border: 1px solid #ddd;
            background: #111;
            color: #0f0;
            white-space: pre-wrap;
            overflow-y: auto; /* 确保垂直滚动 */
            -webkit-overflow-scrolling: touch;
            line-height: 1.5;
            resize: none; /* 防止用户调整大小 */
        }
        
        /* Status indicators */
        .status-indicator {
            color: #FFA500;
            font-style: italic;
            margin: 8px 0;
            font-size: 14px;
        }
        
        /* Loading overlay optimized for mobile */
        #loadingOverlay {
            position: fixed;
            top: 0;
            left: 0;
            width: 100%;
            height: 100%;
            background: rgba(0, 0, 0, 0.85);
            display: flex;
            justify-content: center;
            align-items: center;
            z-index: 1000;
            color: white;
            font-size: 1.2rem;
            flex-direction: column;
            padding: 20px;
            text-align: center;
        }
        #loadingMessage {
            margin-top: 20px;
            font-size: 1rem;
        }
        
        /* API Key Form */
        #apiKeyForm {
            display: none;
            margin-top: 20px;
            background: rgba(255, 255, 255, 0.1);
            padding: 20px;
            border-radius: 10px;
            width: 90%;
            max-width: 500px;
        }
        #apiKeyInput {
            width: 100%;
            padding: 14px;
            margin: 12px 0;
            border-radius: 8px;
            border: 1px solid #ddd;
            font-size: 16px;
            background: white;
            -webkit-appearance: none;
        }
        #submitApiKey {
            padding: 14px 20px;
            background: #27ae60;
            color: white;
            border: none;
            border-radius: 8px;
            cursor: pointer;
            font-size: 16px;
            font-weight: 600;
        }
        #submitApiKey:active {
            background: #219653;
        }
        .api-key-label {
            font-size: 16px;
            margin-bottom: 12px;
            display: block;
        }
        
        /* OpenAI API Key Form */
        #openaiKeyForm {
            display: none;
            margin-top: 20px;
            background: rgba(255, 255, 255, 0.1);
            padding: 20px;
            border-radius: 10px;
            width: 90%;
            max-width: 500px;
        }
        #openaiKeyInput {
            width: 100%;
            padding: 14px;
            margin: 12px 0;
            border-radius: 8px;
            border: 1px solid #ddd;
            font-size: 16px;
            background: white;
            -webkit-appearance: none;
        }
        #submitOpenaiKey {
            padding: 14px 20px;
            background: #10a37f;
            color: white;
            border: none;
            border-radius: 8px;
            cursor: pointer;
            font-size: 16px;
            font-weight: 600;
        }
        #submitOpenaiKey:active {
            background: #0d8c6c;
        }
        .openai-key-label {
            font-size: 16px;
            margin-bottom: 12px;
            display: block;
        }
        .openai-key-info {
            font-size: 13px;
            color: #aaa;
            margin-top: 8px;
        }
        
        /* Performance warning */
        .performance-warning {
            background: #f39c12;
            color: white;
            padding: 10px;
            border-radius: 8px;
            margin-bottom: 15px;
            text-align: center;
            font-size: 14px;
            display: none;
        }
        
        /* Transformers.js loading indicator */
        #transformersLoading {
            display: none;
            position: fixed;
            top: 50%;
            left: 50%;
            transform: translate(-50%, -50%);
            background: rgba(0, 0, 0, 0.9);
            color: white;
            padding: 20px;
            border-radius: 10px;
            z-index: 1001;
            text-align: center;
        }
        #transformersProgress {
            margin-top: 10px;
            font-size: 14px;
        }
        .progress-bar {
            width: 100%;
            height: 6px;
            background: #333;
            border-radius: 3px;
            margin-top: 10px;
            overflow: hidden;
        }
        .progress-fill {
            height: 100%;
            background: #3498db;
            width: 0%;
            transition: width 0.3s ease;
        }
        
        /* Responsive adjustments */
        @media (max-width: 480px) {
            #outputBox {
                height: 50vh;
                max-height: 50vh;
            }
        }
        
        /* Animation optimizations */
        .typing-cursor {
            display: inline-block;
            background-color: #0f0;
            width: 6px;
            height: 14px;
            animation: blink 1s infinite;
            margin-left: 2px;
            vertical-align: middle;
        }
        @keyframes blink {
            0%, 100% { opacity: 1; }
            50% { opacity: 0; }
        }
        
        /* Simplified dot animation for mobile */
        .dot-flashing {
            display: inline-block;
            width: 8px;
            height: 8px;
            border-radius: 4px;
            background-color: #FFA500;
            animation: dotFlashing 1s infinite linear alternate;
            margin-left: 4px;
        }
        @keyframes dotFlashing {
            0% { opacity: 1; }
            100% { opacity: 0.3; }
        }
    </style>
</head>
<body>
    <div class="performance-warning" id="perfWarning">
        ⚠️ 移动设备性能有限，加载可能需要更长时间
    </div>
    
    <div class="container">
        <h2>Warsonco客服机器人</h2>
        
        <div class="input-group">
            <input id="inputBox" type="text" placeholder="输入你的问题..." disabled />
            <button id="sendButton" disabled>发送</button>
        </div>

        <div id="outputBox"></div>
    </div>

    <div id="loadingOverlay">
        <div>正在加载AI助手...</div>
        <div id="loadingMessage">🤖 正在初始化智能体...</div>
        
        <!-- API Key Input Form -->
        <div id="apiKeyForm">
            <span class="api-key-label">🔑 请输入您的DeepSeek API密钥:</span>
            <input type="password" id="apiKeyInput" placeholder="sk-xxxxxxxxxxxxxxxx" />
            <button id="submitApiKey">提交</button>
        </div>
        
        <!-- OpenAI API Key Input Form -->
        <div id="openaiKeyForm">
            <span class="openai-key-label">🔑 需要OpenAI API密钥用于数据检索:</span>
            <input type="password" id="openaiKeyInput" placeholder="sk-xxxxxxxxxxxxxxxx" />
            <button id="submitOpenaiKey">提交</button>
            <div class="openai-key-info">Transformers.js加载失败，需要使用OpenAI的嵌入API作为备用方案</div>
        </div>
    </div>

    <!-- Transformers.js loading indicator -->
    <div id="transformersLoading">
        <div>加载Transformers.js模型...</div>
        <div id="transformersProgress">0%</div>
        <div class="progress-bar">
            <div class="progress-fill" id="progressFill"></div>
        </div>
    </div>

    <script type="module">
        // Save OpenAI key globally for checking
        window.openaiKeyAvailable = false;
        window.needOpenaiKey = false;
        
        // Use a lighter model for mobile devices with quantization
        const isMobile = /Android|webOS|iPhone|iPad|iPod|BlackBerry|IEMobile|Opera Mini/i.test(navigator.userAgent);
        const modelName = 'Xenova/all-MiniLM-L6-v2'; // Lightweight model for mobile
        
        // Show loading indicator for Transformers.js
        document.getElementById('transformersLoading').style.display = 'block';
        
        import { pipeline, env } from 'https://gcore.jsdelivr.net/npm/@xenova/transformers@latest';
        
        // Configure environment for mobile optimization
        env.allowRemoteModels = true;
        env.backends.onnx.wasm.numThreads = isMobile ? 1 : 2; // Reduce threads on mobile
        env.backends.onnx.wasm.proxy = true;
        
        // Update progress indicator
        let lastProgressUpdate = 0;
        const progressCallback = (data) => {
            if (data.status === 'progress') {
                const now = Date.now();
                // Throttle progress updates to avoid UI jank
                if (now - lastProgressUpdate > 200) {
                    const percent = Math.round(data.progress * 100);
                    document.getElementById('transformersProgress').textContent = `${percent}%`;
                    document.getElementById('progressFill').style.width = `${percent}%`;
                    lastProgressUpdate = now;
                }
            }
        };
        
        window.embeddingGenerator = null;

        // async function checkNetwork() {
        //     try {
        //         const testResponse = await fetch('https://api.deepseek.com', { method: 'HEAD', timeout: 5000 });
        //         if (!testResponse.ok) throw new Error('Network test failed');
        //         console.log('Network to DeepSeek API is healthy');
        //         return true;
        //     } catch (error) {
        //         console.error('Network check failed:', error);
        //         return false;
        //     }
        // }

        async function initTransformers() {
            try {
                console.log('Loading Transformers.js pipeline with mobile optimization...');
                
                // Use a simpler configuration for mobile with quantization
                const config = {
                    quantized: true, // Use quantized model for mobile
                    progress_callback: progressCallback,
                    device: isMobile ? "wasm" : "webgpu", // Use WASM on mobile for stability
                    dtype: {
                        encoder_model: isMobile ? "q4" : "fp32" // Quantize more on mobile
                    }
                };
                
                window.embeddingGenerator = await pipeline('feature-extraction', modelName, config);
                console.log("Transformers.js pipeline created successfully");
                
                // Hide loading indicator
                document.getElementById('transformersLoading').style.display = 'none';
                
                return true;
            } catch (error) {
                console.error("Failed to initialize Transformers.js:", error);
                // Hide loading indicator even on error
                document.getElementById('transformersLoading').style.display = 'none';
                return false;
            }
        }

        // Initialize Transformers.js with a timeout to prevent hanging 
        const transformersInitPromise = initTransformers();
        
        // Set a timeout to prevent the initialization from hanging indefinitely
        const timeoutPromise = new Promise((resolve) => {
            setTimeout(() => {
                // Show error message but continue
                document.getElementById('transformersLoading').style.display = 'none';
                const loadingMessage = document.getElementById('loadingMessage');
                if (loadingMessage) {
                    loadingMessage.innerHTML = 'Transformers.js加载超时，将尝试使用备用模式 <span class="dot-flashing"></span>';
                }
                resolve(false);
            }, isMobile ? 90000 : 45000); // 90 seconds timeout on mobile, 45 on desktop
        });

        // Race between initialization and timeout
        Promise.race([transformersInitPromise, timeoutPromise]).then(success => {
            window.transformersInitialized = success;
            if (success) {
                console.log("Transformers.js initialized successfully");
            } else {
                console.warn("Transformers.js initialization failed or timed out");
                // Check if OpenAI key is available in config
                checkOpenAIKey();
            }
        }).catch(error => {
            console.error("Error in Transformers.js initialization:", error);
            window.transformersInitialized = false;
            document.getElementById('transformersLoading').style.display = 'none';
            // Check if OpenAI key is available
            checkOpenAIKey();
        });
        
        function checkOpenAIKey() {
            // Wait for config to load
            if (!window.APP_CONFIG) {
                setTimeout(checkOpenAIKey, 100);
                return;
            }
            
            const openaiKey = window.APP_CONFIG.OPENAI_API_KEY;
            if (openaiKey && openaiKey !== "your_openai_api_key_here") {
                window.openaiKeyAvailable = true;
                console.log("OpenAI API key found in config, will use for embeddings");
            } else {
                window.needOpenaiKey = true;
                console.log("No OpenAI API key found, will prompt user if needed");
            }
        }
    </script>

    <script type="text/javascript">
      // Show performance warning on mobile
      if (/Android|webOS|iPhone|iPad|iPod|BlackBerry|IEMobile|Opera Mini/i.test(navigator.userAgent)) {
        document.getElementById('perfWarning').style.display = 'block';
      }
      
      let pyodide;
      let isStreaming = false;
      let currentStream = null;
      let apiKeyProvided = false;
      let openaiKeyProvided = false;
      let isMobile = /Android|webOS|iPhone|iPad|iPod|BlackBerry|IEMobile|Opera Mini/i.test(navigator.userAgent);
      let lastUserInput = "";
      window.llmTimeout = isMobile ? 180 : 60;
      
      // Function to use OpenAI embeddings
      async function getOpenAIEmbedding(text) {
        const controller = new AbortController();
        const timeoutId = setTimeout(() => controller.abort(), 30000);

        const openaiKey = window.APP_CONFIG.OPENAI_API_KEY;
        if (!openaiKey) {
          throw new Error("No OpenAI API key available");
        }
        
        try {
          const response = await fetch('https://api.openai.com/v1/embeddings', {
            method: 'POST',
            signal: controller.signal,
            headers: {
              'Authorization': `Bearer ${openaiKey}`,
              'Content-Type': 'application/json',
            },
            body: JSON.stringify({
              input: text,
              model: "text-embedding-3-small",
              dimensions: 384
            })
          });
          clearTimeout(timeoutId);
          
          if (!response.ok) {
            throw new Error(`OpenAI API error: ${response.status}`);
          }
          
          const data = await response.json();
          return data.data[0].embedding;
        } catch (error) {
          clearTimeout(timeoutId);
          if (error.name === 'AbortError') {
            throw new Error("OpenAI embedding timeout");
          }
          throw error;
        }
      }
      
      // Function to update status from Python
      function updateStatus(message) {
        showStatus(message + " <span class='dot-flashing'></span>");
        
        // Also update the loading message if overlay is still visible
        const loadingMessage = document.getElementById('loadingMessage');
        if (loadingMessage) {
          loadingMessage.textContent = message;
        }
      }

      // Function to check if API key is provided
      function checkApiKey() {
        // Wait for config to load
        if (!window.APP_CONFIG) {
          setTimeout(checkApiKey, 100);
          return;
        }
        
        const JS_DEEPSEEK_API_KEY = window.APP_CONFIG.DEEPSEEK_API_KEY;
        
        // If no API key in config, show the input form
        if (!JS_DEEPSEEK_API_KEY || JS_DEEPSEEK_API_KEY === "your_deepseek_api_key_here") {
          document.getElementById('apiKeyForm').style.display = 'block';
          document.getElementById('loadingMessage').textContent = '需要DeepSeek API密钥才能继续';
        } else {
          // API key exists in config, proceed with initialization
          apiKeyProvided = true;
          main();
        }
      }

      // Handle API key submission
      document.getElementById('submitApiKey').addEventListener('click', function() {
        const apiKey = document.getElementById('apiKeyInput').value.trim();
        if (apiKey) {
          // Set the API key in the config
          if (!window.APP_CONFIG) window.APP_CONFIG = {};
          window.APP_CONFIG.DEEPSEEK_API_KEY = apiKey;
          apiKeyProvided = true;
          
          // Hide the form and continue initialization
          document.getElementById('apiKeyForm').style.display = 'none';
          document.getElementById('loadingMessage').textContent = '🤖 正在初始化智能体...';
          main();
        } else {
          alert('请输入有效的API密钥');
        }
      });

      // Also allow Enter key to submit
      document.getElementById('apiKeyInput').addEventListener('keypress', function(e) {
        if (e.key === 'Enter') {
          document.getElementById('submitApiKey').click();
        }
      });
      
      // Handle OpenAI API key submission
      document.getElementById('submitOpenaiKey').addEventListener('click', async function() {
        const apiKey = document.getElementById('openaiKeyInput').value.trim();
        if (apiKey) {
          // Set the OpenAI API key in the config
          if (!window.APP_CONFIG) window.APP_CONFIG = {};
          window.APP_CONFIG.OPENAI_API_KEY = apiKey;
          window.openaiKeyAvailable = true;
          openaiKeyProvided = true;
          
          await pyodide.runPythonAsync(`
            import os
            os.environ['OPENAI_API_KEY'] = "${apiKey}"
          `);
          
          // Hide the form
          document.getElementById('openaiKeyForm').style.display = 'none';
          
          if (document.getElementById('loadingOverlay').style.display === 'flex' && window.transformersInitialized === false) {
            // Init phase for embeddings
            document.getElementById('loadingMessage').textContent = '🤖 正在初始化智能体...';
            continueMainInit();
          } else {
            // Runtime fallback for LLM
            document.getElementById('loadingOverlay').style.display = 'none';
            hideStatus();
            
            await pyodide.runPythonAsync(`
              from langchain_openai import ChatOpenAI
              fallback_llm = ChatOpenAI(model="gpt-4.1-nano", temperature=0, request_timeout=${window.llmTimeout}) # gpt-4o-mini
              agent_instance.fallback_llm = fallback_llm
              print("Fallback LLM set")
            `);
            
            showStatus("使用OpenAI重试 <span class='dot-flashing'></span>");
            
            const retryResult = await pyodide.runPythonAsync(`
              await main_function(${JSON.stringify(lastUserInput)})
            `);
            
            hideStatus();
            
            streamOutput("🤖 助手: " + retryResult, "assistant");
          }
        } else {
          alert('请输入有效的OpenAI API密钥');
        }
      });

      // Also allow Enter key to submit OpenAI key
      document.getElementById('openaiKeyInput').addEventListener('keypress', function(e) {
        if (e.key === 'Enter') {
          document.getElementById('submitOpenaiKey').click();
        }
      });

      async function main(){
        // Show loading overlay initially
        document.getElementById('loadingOverlay').style.display = 'flex';
        
        // Wait for config and API key
        while (!window.APP_CONFIG || !apiKeyProvided) {
          await new Promise(r => setTimeout(r, 50));
        }
        
        const JS_LLM_MODE = window.APP_CONFIG.LLM_MODE;
        const JS_DEEPSEEK_API_KEY = window.APP_CONFIG.DEEPSEEK_API_KEY;
        const JS_HUGGINGFACEHUB_API_TOKEN = window.APP_CONFIG.HUGGINGFACEHUB_API_TOKEN;
        const JS_TAVILY_API_KEY = window.APP_CONFIG.TAVILY_API_KEY;
        
        // Default to transformersjs for mobile if not specified
        let JS_EMBEDDING_MODE = window.APP_CONFIG.EMBEDDING_MODE || (isMobile ? "transformersjs" : "local");

        // Wait for Transformers.js to initialize if we're using it
        if (JS_EMBEDDING_MODE === "transformersjs") {
          updateStatus("🔄 正在加载Transformers.js模型...");
          
          // Show Transformers.js loading indicator
          document.getElementById('transformersLoading').style.display = 'block';
          
          // Wait for Transformers.js to initialize or timeout
          let waitTime = 0;
          while (window.transformersInitialized === undefined && waitTime < 90000) {
            await new Promise(r => setTimeout(r, 100));
            waitTime += 100;
          }
          
          // Hide Transformers.js loading indicator
          document.getElementById('transformersLoading').style.display = 'none';
          
          if (!window.transformersInitialized) {
            console.warn("Transformers.js failed to initialize, checking for OpenAI fallback");
            
            // Check if OpenAI key is available
            if (window.openaiKeyAvailable) {
              JS_EMBEDDING_MODE = "openai";
              window.APP_CONFIG.EMBEDDING_MODE = "openai";
              console.log("Using OpenAI embeddings as fallback");
            } else if (window.needOpenaiKey) {
              // Need to prompt for OpenAI key
              document.getElementById('openaiKeyForm').style.display = 'block';
              document.getElementById('loadingMessage').textContent = '需要OpenAI API密钥用于嵌入向量';
              
              // Wait for OpenAI key to be provided
              while (!openaiKeyProvided) {
                await new Promise(r => setTimeout(r, 100));
              }
              
              JS_EMBEDDING_MODE = "openai";
              window.APP_CONFIG.EMBEDDING_MODE = "openai";
            } else {
              // Fall back to local mode
              JS_EMBEDDING_MODE = "local";
              window.APP_CONFIG.EMBEDDING_MODE = "local";
              console.log("Using local embeddings as fallback");
            }
          }
        }
        
        // Continue with initialization
        await continueMainInit();
      }
      
      async function continueMainInit() {
        const JS_LLM_MODE = window.APP_CONFIG.LLM_MODE;
        const JS_DEEPSEEK_API_KEY = window.APP_CONFIG.DEEPSEEK_API_KEY;
        const JS_HUGGINGFACEHUB_API_TOKEN = window.APP_CONFIG.HUGGINGFACEHUB_API_TOKEN;
        const JS_TAVILY_API_KEY = window.APP_CONFIG.TAVILY_API_KEY;
        const JS_OPENAI_API_KEY = window.APP_CONFIG.OPENAI_API_KEY || "";
        const JS_EMBEDDING_MODE = window.APP_CONFIG.EMBEDDING_MODE;

        // Set dynamic timeout based on device
        const llmTimeout = isMobile ? 180 : 60;  // 3min on mobile, 1min on PC
        window.APP_CONFIG.LLM_TIMEOUT = llmTimeout;
        
        // Initialize Pyodide with a progress callback
        updateStatus("🔄 正在加载Pyodide...");
        pyodide = await loadPyodide({
          // Show loading progress for better UX
          stdout: msg => console.log(msg),
          stderr: msg => console.error(msg)
        });
        
        // Register the updateStatus function in Python's global scope
        pyodide.globals.set("updateStatus", updateStatus);
        // Pass embedding mode to Python
        pyodide.globals.set("JS_EMBEDDING_MODE", JS_EMBEDDING_MODE);

        // --- 1. Fetch embeddings.npy with progress tracking ---
        updateStatus("📥 正在加载嵌入数据...");
        try {
          const npyResp = await fetchWithProgress("https://mofanv.github.io/dist/embeddings.npy", 
            "embeddings.npy", "嵌入数据");
          if (!npyResp.ok) throw new Error("加载RAG Embedding数据文件失败，请查看是否有访问权限。");
          const npyBuffer = await npyResp.arrayBuffer();
          pyodide.FS.writeFile("/embeddings.npy", new Uint8Array(npyBuffer));
        } catch (error) {
          console.error("Error loading embeddings:", error);
          updateStatus("❌ 加载嵌入数据失败");
          // Continue with a fallback - create empty embeddings
          const fallbackEmbeddings = new Float32Array(384); // Default size
          pyodide.FS.writeFile("/embeddings.npy", fallbackEmbeddings);
        }

        // --- 2. Fetch embeddings_docs.json with progress tracking ---
        updateStatus("📥 正在加载文档数据...");
        try {
          const jsonResp = await fetchWithProgress("https://mofanv.github.io/dist/embeddings_docs.json", 
            "embeddings_docs.json", "文档数据");
          if (!jsonResp.ok) throw new Error("加载RAG Docs数据文件失败，请查看是否有访问权限。");
          const jsonText = await jsonResp.text();
          pyodide.FS.writeFile("/embeddings_docs.json", jsonText);
        } catch (error) {
          console.error("Error loading docs:", error);
          updateStatus("❌ 加载文档数据失败");
          // Continue with a fallback - create empty docs
          const fallbackDocs = JSON.stringify([{text: "No data available", metadata: {url: "N/A"}}]);
          pyodide.FS.writeFile("/embeddings_docs.json", fallbackDocs);
        }

        updateStatus("📦 正在安装Python包...");
        await pyodide.loadPackage("micropip");

        // Install only essential packages for mobile 
        const packagesToInstall = ["requests==2.32.5", "numpy", "langchain", "langchain_deepseek", "langchain_openai"];

        console.log(await pyodide.runPythonAsync(`
            import sys
            sys.version

            import micropip

            await micropip.install(${JSON.stringify(packagesToInstall)}, keep_going=True)
        `));

        await pyodide.runPythonAsync(`
            import os
            os.environ['DEEPSEEK_API_KEY'] = "${JS_DEEPSEEK_API_KEY}"
            os.environ['HUGGINGFACEHUB_API_TOKEN'] = "${JS_HUGGINGFACEHUB_API_TOKEN}"
            os.environ['OPENAI_API_KEY'] = "${JS_OPENAI_API_KEY}"
            os.environ['LLM_MODE'] = "${JS_LLM_MODE}"
            os.environ['EMBEDDING_MODE'] = "${JS_EMBEDDING_MODE}"
            print("API Key set:", os.environ['DEEPSEEK_API_KEY'][:6] + "...")
            print("LLM Mode:", os.environ['LLM_MODE'])
            print("Embedding Mode:", os.environ['EMBEDDING_MODE'])
            if os.environ.get('OPENAI_API_KEY'):
                print("OpenAI Key available:", os.environ['OPENAI_API_KEY'][:6] + "...")
        `);
        
        // Now run the main Python code
        updateStatus("🐍 正在执行Python代码...");
        
        // Use a simplified Python code for mobile to improve performance 
        const pythonCode = getPythonCode(JS_EMBEDDING_MODE);
        
        await pyodide.runPythonAsync(pythonCode);
        
        // Hide the loading overlay and enable input
        document.getElementById('loadingOverlay').style.display = 'none';
        document.getElementById('inputBox').disabled = false;
        document.querySelector('button').disabled = false;
        document.getElementById('inputBox').focus();
        
        // Clear any status messages in the chat output
        hideStatus();
      }

        // Function to fetch with progress tracking
        async function fetchWithProgress(url, filename, description) {
        const response = await fetch(url);
        const reader = response.body.getReader();
        const chunks = [];
        let loaded = 0;
        let startTime = Date.now();
        let lastUpdateTime = startTime;
        let lastLoaded = 0;

        while(true) {
            const {done, value} = await reader.read();
            if (done) break;
            
            chunks.push(value);
            loaded += value.length;
            
            const currentTime = Date.now();
            const timeElapsed = (currentTime - lastUpdateTime) / 1000; // in seconds
            
            // Update every 500ms or when 1MB is downloaded
            if (timeElapsed > 0.5 || loaded - lastLoaded >= 1024 * 1024) {
            const downloadSpeed = (loaded - lastLoaded) / timeElapsed; // bytes per second
            const speedText = formatBytes(downloadSpeed) + '/s';
            
            updateStatus(`📥 正在加载${description}... ${formatBytes(loaded)} (${speedText})`);
            
            lastUpdateTime = currentTime;
            lastLoaded = loaded;
            }
        }
        
        // Final download complete message
        const totalTime = (Date.now() - startTime) / 1000;
        const averageSpeed = loaded / totalTime;
        const speedText = formatBytes(averageSpeed) + '/s';
        updateStatus(`✅ ${description}加载完成! ${formatBytes(loaded)} (平均速度: ${speedText})`);

        // Combine chunks
        const blob = new Blob(chunks);
        return new Response(blob);
        }

        // Helper function to format bytes to human-readable format
        function formatBytes(bytes, decimals = 2) {
        if (bytes === 0) return '0 B';
        
        const k = 1024;
        const sizes = ['B', 'KB', 'MB', 'GB'];
        const i = Math.floor(Math.log(bytes) / Math.log(k));
        
        return parseFloat((bytes / Math.pow(k, i)).toFixed(decimals)) + ' ' + sizes[i];
        }
      
      // Get appropriate Python code based on embedding mode
      function getPythonCode(embeddingMode) {
        // Mobile-optimized Python code with better error handling
        return `
import json
import os
import numpy as np
from langchain.schema import Document
from langchain.tools import tool
from langchain.memory import ConversationBufferMemory
from langchain.agents import initialize_agent, AgentType
from langchain_deepseek import ChatDeepSeek
from langchain_openai import ChatOpenAI

# ===================== LLM SELECTION =====================
LLM_MODE = os.getenv("LLM_MODE", "deepseek")
EMBEDDING_MODE = os.getenv("EMBEDDING_MODE", "local")
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY", "")

timeout_val = int(os.getenv("LLM_TIMEOUT", "60"))  # Defaults to 60 if not set

if LLM_MODE == "deepseek":
    llm = ChatDeepSeek(model="deepseek-chat", timeout=timeout_val)
    print(f"Using DeepSeek with timeout: {timeout_val}s")
    print("Using DeepSeek as the Agent LLM...")
else:
    raise ValueError(f"Unsupported LLM_MODE: {LLM_MODE}")

fallback_llm = None
if OPENAI_API_KEY:
    fallback_llm = ChatOpenAI(model="gpt-3.5-turbo", temperature=0, request_timeout=timeout_val)
    print("OpenAI fallback LLM initialized")

print(f"Using embedding mode: {EMBEDDING_MODE}")

use_openai = False

# ===================== KNOWLEDGE BASE =====================
EMBEDDINGS_FILE = "/embeddings.npy"
DOCS_FILE = "/embeddings_docs.json"

# Load precomputed embeddings
try:
    vectors = np.load(EMBEDDINGS_FILE, allow_pickle=True)
    vectors = np.array(vectors, dtype=np.float32)
    # Normalize vectors
    norms = np.linalg.norm(vectors, axis=1, keepdims=True)
    vectors /= (norms + 1e-10)
except:
    vectors = np.zeros((1, 384), dtype=np.float32)  # Fallback

# Load metadata/docs
try:
    with open(DOCS_FILE, "r", encoding="utf-8") as f:
        docs_store = [Document(page_content=d["text"], metadata=d["metadata"]) for d in json.load(f)]
except:
    docs_store = [Document(page_content="No data available", metadata={})]  # Fallback




def get_embedding(text: str):
    """Get embedding based on mode"""
    updateStatus("🔍 正在生成嵌入向量...")
    try:
        if EMBEDDING_MODE == "transformersjs":
            try:
                # Use Transformers.js embedding from JavaScript
                import numpy as np
                emb = np.array(js_embedding, dtype=np.float32)
                # Normalize
                norm = np.linalg.norm(emb)
                if norm > 0:
                    emb /= norm
                return emb
            except NameError:
                print("[Warning] js_embedding not available, falling back")
                pass  # Fall through to next
        elif EMBEDDING_MODE == "openai":
            try:
                # Use OpenAI embedding from JavaScript
                import numpy as np
                emb = np.array(js_openai_embedding, dtype=np.float32)
                # Normalize
                norm = np.linalg.norm(emb)
                if norm > 0:
                    emb /= norm
                return emb
            except NameError:
                print("[Warning] js_openai_embedding not available, falling back")
                pass  # Fall through to next
        
        # Fallback to simple embedding only if above fails (e.g., no JS var set)
        import numpy as np
        # Simple hash-based embedding for mobile fallback
        words = text.lower().split()
        emb = np.zeros(384, dtype=np.float32)
        count = 0
        for word in words:
            if len(word) > 2:  # Skip very short words
                seed = hash(word) % 1000
                np.random.seed(seed)
                emb += np.random.rand(384).astype(np.float32) - 0.5
                count += 1
        if count > 0:
            emb /= count
            norm = np.linalg.norm(emb)
            if norm > 0:
                emb /= norm
        return emb
    except Exception as e:
        print(f"[Embedding Error] {e}")
        return np.zeros(vectors.shape[1], dtype=np.float32) if vectors.size > 0 else np.zeros(384, dtype=np.float32)

        
    
def query_kb(query: str, k: int = 3):
    """Query the knowledge base"""
    updateStatus("📚 正在搜索知识库...")
    q_vec = get_embedding(query)
    
    # Check if vectors are available
    if vectors.size == 0 or vectors.shape[1] != q_vec.shape[0]:
        updateStatus("⚠️ 知识库未加载，使用默认回复")
        return [Document(page_content="知识库暂不可用，请稍后再试", metadata={"url": "N/A"})]
    
    try:
        sims = np.dot(q_vec, vectors.T)[0]
        top_idx = sims.argsort()[-k:][::-1]
        updateStatus("✅ 已找到相关文档")
        return [docs_store[i] for i in top_idx]
    except:
        # Return empty results on error
        return []

# ===================== TOOLS =====================
@tool
def search_knowledge(query: str):
    """检索知识库相关信息"""
    updateStatus("🔎 正在搜索知识库...")
    docs = query_kb(query, k=3)
    return [f"{d.page_content}\\n来源: {d.metadata.get('url','未知来源')}" for d in docs]

@tool
def get_price_info(query: str):
    """检索价格相关信息"""
    updateStatus("💰 正在搜索价格信息...")
    docs = query_kb(query, k=3)
    price_docs = [d for d in docs if "价" in d.page_content or "￥" in d.page_content]
    return [f"{d.page_content}\\n来源: {d.metadata.get('url','未知来源')}" for d in price_docs] if price_docs else "未找到相关价格信息"

# ===================== AGENT SETUP =====================
tools = [search_knowledge, get_price_info]

system_prompt = """你是【华盛控科技智能客服助手】，职责是为潜在客户和合作伙伴提供专业、简洁、可靠的支持。

### 你的能力与边界
1. **信息来源**  
   - 优先使用知识库（官网内容、产品手册、常见问题、售后政策等）回答。  
   - 如果知识库没有相关信息，请坦诚说明，并礼貌引导用户联系人工客服。  

2. **回答规则**  
   - 回答要清晰、简洁、分点列出，便于客户快速理解。  
   - 仅提供与公司、产品、服务相关的内容，不回答无关话题。  
   - 回答时避免冗长和空话。  

3. **交互风格**  
   - 保持专业、友好、耐心。  
   - 如果问题模糊，请主动追问澄清。  
   - 优先使用客户输入的语言回答（中文或英文）。  

4. **场景引导**  
   - **产品咨询**：可推荐产品、列出参数对比，并引导用户下载资料或联系销售。  
   - **售后支持**：提供排查建议；如果超出能力范围，提示报修或联系售后。  
   - **方案/报价需求**：收集需求信息（行业、产能、联系方式等），并建议后续人工对接。  
   - **联系方式**：可提供公司电话、邮箱、在线客服入口等。  

5. **额外要求**  
   - 在涉及购买、报价、方案定制时，要引导用户留下联系方式。  
   - 遇到重复问题时，保持耐心并尝试换种表述方式。  
"""

memory = ConversationBufferMemory(
    memory_key="chat_history",
    return_messages=True,
)

updateStatus("🤖 正在初始化智能体...")
is_mobile = globals().get("JS_IS_MOBILE", False)
agent = initialize_agent(
    tools,
    llm,
    agent=AgentType.CHAT_CONVERSATIONAL_REACT_DESCRIPTION,
    memory=memory,
    verbose=False,  # Reduced verbosity for mobile
    max_iterations=1 if is_mobile else 2,  # Reduce on mobile
    early_stopping_method="generate"
)

# ===================== AGENT INTERFACE =====================
class ProductAssistantAgent:
    def __init__(self, agent_llm, fallback_llm=None):
        self.agent = agent_llm
        self.fallback_llm = fallback_llm

    def is_input_relevant(self, user_input: str, threshold=0.2):
        updateStatus("🔍 正在检查查询相关性...")
        q_vec = get_embedding(user_input)
        
        if vectors.size == 0:
            print("[Debug] Vectors empty, assuming relevant")
            return True
            
        vecs = vectors.astype(np.float32)
        print(f"[Debug] q_vec shape: {q_vec.shape}, vecs shape: {vecs.shape}")
        
        if q_vec.shape[0] != vecs.shape[1]:
            print("[Debug] Shape mismatch!")
            return False

        try:
            updateStatus("📊 正在计算相似度分数...")
            sims = np.dot(q_vec, vecs.T)
            print(f"[Debug] sims shape: {sims.shape}, max_sim: {np.max(sims)}")
            if sims.size == 0:
                return False
            max_sim = float(np.max(sims))
            print(f"[Debug] max_sim {max_sim} >= {threshold}: {max_sim >= float(threshold)}")
            return max_sim >= float(threshold)
        except (TypeError, ValueError) as e:
            print(f"[Debug] Sim error: {e}")
            return False


    async def process_message(self, message: str):
        global use_openai
        if not self.is_input_relevant(message):
            return "抱歉，我不太理解您的问题。可以请您描述得更清楚吗？"

        updateStatus("🧠 正在思考如何回答...")
        if use_openai and self.fallback_llm:
            updateStatus("🧠 使用 OpenAI 思考...")
            fallback_agent = initialize_agent(
                tools,
                self.fallback_llm,
                agent=AgentType.CHAT_CONVERSATIONAL_REACT_DESCRIPTION,
                memory=memory,
                verbose=False,
                max_iterations=1 if is_mobile else 2,
                early_stopping_method="generate"
            )
            try:
                ai_raw = fallback_agent.invoke({
                    "input": f"""{system_prompt}\n\n用户问题: {message}""",
                    "chat_history": memory.load_memory_variables({})["chat_history"]
                })
                success = True
            except Exception as fallback_e:
                print(f"[Fallback Error] {fallback_e}")
                return "备用 OpenAI API 也出现问题，请检查网络并稍后重试。"
        else:
            max_retries = 2
            success = False
            ai_raw = None
            for attempt in range(max_retries):
                try:
                    # Add explicit timeout to llm call if possible
                    ai_raw = self.agent.invoke({
                        "input": f"""{system_prompt}\n\n用户问题: {message}""",
                        "chat_history": memory.load_memory_variables({})["chat_history"]
                    })
                    success = True
                    break
                except Exception as e:
                    error_str = str(e).lower()
                    if "timeout" in error_str or "connection" in error_str:
                        if attempt < max_retries - 1:
                            wait_time = 2 ** attempt
                            print(f"[Retry {attempt+1}/{max_retries}] Timeout: {e}. Waiting {wait_time}s...")
                            import asyncio
                            await asyncio.sleep(wait_time)
                            continue
                    elif "invalid response" in error_str or "json" in error_str:
                        if attempt < max_retries - 1:
                            wait_time = 2 ** attempt
                            print(f"[Retry {attempt+1}/{max_retries}] Invalid response. Waiting {wait_time}s...")
                            import asyncio
                            await asyncio.sleep(wait_time)
                            continue
                        else:
                            break  # Will trigger fallback below
                    else:
                        raise e

            if not success:
                use_openai = True
                if self.fallback_llm:
                    updateStatus("⚠️ DeepSeek API 失败，切换到 OpenAI...")
                    fallback_agent = initialize_agent(
                        tools,
                        self.fallback_llm,
                        agent=AgentType.CHAT_CONVERSATIONAL_REACT_DESCRIPTION,
                        memory=memory,
                        verbose=False,
                        max_iterations=1 if is_mobile else 2,
                        early_stopping_method="generate"
                    )
                    try:
                        ai_raw = fallback_agent.invoke({
                            "input": f"""{system_prompt}\n\n用户问题: {message}""",
                            "chat_history": memory.load_memory_variables({})["chat_history"]
                        })
                        success = True
                    except Exception as fallback_e:
                        print(f"[Fallback Error] {fallback_e}")
                        return "备用 OpenAI API 也出现问题，请检查网络并稍后重试。"
                else:
                    return "NEED_OPENAI_KEY"

        if not success:
            return "API响应无效，请检查网络或API密钥。建议使用Wi-Fi重试。"

        # Extract response
        if hasattr(ai_raw, "content"):
            ai_response = ai_raw.content
        elif isinstance(ai_raw, dict) and "output" in ai_raw:
            ai_response = ai_raw["output"]
        else:
            ai_response = str(ai_raw)

        # Handle errors in response content
        if "invalid response" in ai_response.lower() or "api" in ai_response.lower():
            return "网络连接出现问题，请稍后重试或检查API密钥是否正确。"
            
        return ai_response



# ===================== GLOBAL AGENT INSTANCE =====================
agent_instance = ProductAssistantAgent(agent_llm=agent, fallback_llm=fallback_llm)

# ===================== MAIN FUNCTION =====================
async def main_function(user_input: str) -> str:
    updateStatus("🤖 正在处理您的请求...")
    res = await agent_instance.process_message(user_input)
    return res
          `;
      }

        // Attach click event listener for better mobile compatibility
        document.getElementById('sendButton').addEventListener('click', function(event) {
            event.preventDefault();  // Prevent any default touch behavior
            sendToPyodide();
        });

        // Also ensure Enter key works (already there, but confirm)
        document.getElementById("inputBox").addEventListener("keydown", function(event) {
            if (event.key === 'Enter') {
                event.preventDefault();
                sendToPyodide();
            }
        });
      
      // Start checking for API key when page loads
      checkApiKey();

    async function sendToPyodide() {
        
        if (isStreaming) {
            // If already streaming, cancel the current stream
            if (currentStream) {
            clearInterval(currentStream);
            currentStream = null;
            }
            // Remove the cursor
            const cursor = document.querySelector('.typing-cursor');
            if (cursor) cursor.remove();
            isStreaming = false;
        }
        
        const userInput = document.getElementById("inputBox").value;
        if (!userInput.trim()) {
            return;
        }
        
        lastUserInput = userInput;
        
        logOutput("👤 用户: " + userInput, "user");
        
        // Disable input during processing
        document.getElementById("inputBox").value = "";
        document.getElementById("inputBox").disabled = true;
        document.querySelector('button').disabled = true;
        
        // Show initial status
        showStatus("🤖 正在处理您的请求 <span class='dot-flashing'></span>");
        
        try {
            
            // Check which embedding mode to use
            let embeddingMode = window.APP_CONFIG.EMBEDDING_MODE || "local";

            if (embeddingMode === "transformersjs" && window.embeddingGenerator) {
            // Use Transformers.js to generate embeddings
            showStatus("🧠 正在使用 Transformers.js 生成嵌入 <span class='dot-flashing'></span>");
            
            // Generate embedding vector
            const output = await window.embeddingGenerator(userInput, { pooling: 'mean', normalize: true });
            const embeddingArray = Array.from(output.data);
            
            // Pass embedding vector to Pyodide
            pyodide.globals.set("js_embedding", embeddingArray);

            } else if (embeddingMode === "openai") {
            // Use OpenAI to generate embeddings
            showStatus("🧠 正在使用 OpenAI API 生成嵌入 <span class='dot-flashing'></span>");
            
            try {
                const embeddingArray = await getOpenAIEmbedding(userInput);
                // Pass embedding vector to Pyodide
                pyodide.globals.set("js_openai_embedding", embeddingArray);
            } catch (error) {
                console.error("Failed to get OpenAI embedding:", error);
                // Fall back to local mode
                showStatus("⚠️ OpenAI嵌入失败，使用本地模式");
            }
            }
            
            // const networkOk = await checkNetwork();
            // if (!networkOk) {
            //     hideStatus();
            //     logOutput("⚠️ 网络连接不稳定，请连接Wi-Fi并重试。", "error");
            //     document.getElementById("inputBox").disabled = false;
            //     document.querySelector('button').disabled = false;
            //     return;
            // }
            
            // Process user input
            const result = await pyodide.runPythonAsync(`
                await main_function(${JSON.stringify(userInput)})
            `);
            
            if (result === "NEED_OPENAI_KEY") {
              document.getElementById('openaiKeyForm').style.display = 'block';
              document.getElementById('loadingMessage').textContent = 'DeepSeek API 失效，需要OpenAI API密钥作为备用';
              document.getElementById('loadingOverlay').style.display = 'flex';
              document.getElementById('openaiKeyForm').querySelector('.openai-key-info').textContent = 'DeepSeek API 返回无效响应，需要OpenAI作为备用';
              return;
            }
            
            // Clear status
            hideStatus();
            
            // Start streaming the response
            streamOutput("🤖 助手: " + result, "assistant");
        } catch (err) {
            // Clear status
            hideStatus();
            
            logOutput("⚠️ 错误: " + err, "error");
            // Re-enable input on error
            document.getElementById("inputBox").disabled = false;
            document.querySelector('button').disabled = false;
        }
    
    }

      document.getElementById("inputBox").addEventListener("keydown", function(event) {
          if (event.key === 'Enter') {
              event.preventDefault(); // prevent newline in input
              sendToPyodide();
          }
      });

        function logOutput(message, type="assistant") {
            const out = document.getElementById("outputBox");
            
            // 创建消息元素
            const span = document.createElement("span");
            span.textContent = message;

            if (type === "user") {
                span.style.color = "#1E90FF";
                span.style.fontWeight = "bold";
            } else if (type === "assistant") {
                span.style.color = "#32CD32";
            } else if (type === "error") {
                span.style.color = "#FF4500";
                span.style.fontWeight = "bold";
            }

            out.appendChild(span);
            out.appendChild(document.createElement("br"));
            out.appendChild(document.createElement("br"));

            // 确保滚动到底部
            out.scrollTop = out.scrollHeight;
        }
      

        function streamOutput(message, type) {
            isStreaming = true;
            const out = document.getElementById("outputBox");
            
            // 创建消息元素
            const span = document.createElement("span");
            if (type === "assistant") {
                span.style.color = "#32CD32";
            }
            
            out.appendChild(span);
            
            // 创建光标
            const cursor = document.createElement("span");
            cursor.className = "typing-cursor";
            out.appendChild(cursor);
            
            let i = 0;
            currentStream = setInterval(() => {
                if (i < message.length) {
                    span.textContent += message[i];
                    i++;
                    // 每次添加字符时都滚动到底部
                    out.scrollTop = out.scrollHeight;
                } else {
                    clearInterval(currentStream);
                    currentStream = null;
                    isStreaming = false;
                    
                    cursor.remove();
                    
                    out.appendChild(document.createElement("br"));
                    out.appendChild(document.createElement("br"));
                    
                    document.getElementById("inputBox").disabled = false;
                    document.querySelector('button').disabled = false;
                    document.getElementById("inputBox").focus();
                    
                    // 最终滚动到底部
                    out.scrollTop = out.scrollHeight;
                }
            }, 30);
        }
      
      function showStatus(message) {
          // Remove any existing status
          hideStatus();
          
          const out = document.getElementById("outputBox");
          const statusElement = document.createElement("div");
          statusElement.id = "status-indicator";
          statusElement.className = "status-indicator";
          statusElement.innerHTML = message;
          
          out.appendChild(statusElement);
          out.scrollTop = out.scrollHeight;
      }
      
      function hideStatus() {
          const statusElement = document.getElementById("status-indicator");
          if (statusElement) {
              statusElement.remove();
          }
      }

    </script>
</body>
</html>