<html lang="zh-CN">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0">
    <title>Warsoncoå®¢æœæœºå™¨äºº</title>
    <script src="https://cdn.jsdelivr.net/pyodide/v0.28.2/full/pyodide.js"></script>
    <script src="config.js"></script>
    <style>
        /* Base styles optimized for mobile */
        * {
            box-sizing: border-box;
            -webkit-tap-highlight-color: transparent;
        }
        body {
            font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, Oxygen, Ubuntu, sans-serif;
            margin: 0;
            padding: 16px;
            background: #f5f5f5;
            color: #333;
            line-height: 1.6;
            touch-action: manipulation;
            -webkit-font-smoothing: antialiased;
        }
        .container {
            max-width: 100%;
            margin: 0 auto;
        }
        h2 {
            text-align: center;
            color: #2c3e50;
            margin-bottom: 20px;
            font-size: 1.5rem;
        }

        /* Input and button styles optimized for touch */
        .input-group {
            display: flex;
            gap: 10px;
            margin-bottom: 20px;
        }
        #inputBox {
            flex: 1;
            padding: 14px 16px;
            font-size: 16px;
            border-radius: 10px;
            border: 1px solid #ddd;
            background: white;
            min-height: 50px;
            -webkit-appearance: none;
        }
        button {
            padding: 14px 20px;
            font-size: 16px;
            border-radius: 10px;
            border: none;
            background: #3498db;
            color: white;
            cursor: pointer;
            min-width: 80px;
            font-weight: 600;
            touch-action: manipulation;
        }
        button:active {
            background: #2980b9;
            transform: scale(0.98);
        }
        button:disabled {
            background: #95a5a6;
            cursor: not-allowed;
        }

        /* Output box */
        #outputBox {
            width: 100%;
            height: 60vh; /* ä½¿ç”¨è§†å£é«˜åº¦è€Œä¸æ˜¯æœ€å°é«˜åº¦ */
            max-height: 60vh; /* é™åˆ¶æœ€å¤§é«˜åº¦ */
            padding: 16px;
            font-size: 15px;
            border-radius: 10px;
            border: 1px solid #ddd;
            background: #111;
            color: #0f0;
            white-space: pre-wrap;
            overflow-y: auto; /* ç¡®ä¿å‚ç›´æ»šåŠ¨ */
            -webkit-overflow-scrolling: touch;
            line-height: 1.5;
            resize: none; /* é˜²æ­¢ç”¨æˆ·è°ƒæ•´å¤§å° */
        }
        
        /* Status indicators */
        .status-indicator {
            color: #FFA500;
            font-style: italic;
            margin: 8px 0;
            font-size: 14px;
        }
        
        /* Loading overlay optimized for mobile */
        #loadingOverlay {
            position: fixed;
            top: 0;
            left: 0;
            width: 100%;
            height: 100%;
            background: rgba(0, 0, 0, 0.85);
            display: flex;
            justify-content: center;
            align-items: center;
            z-index: 1000;
            color: white;
            font-size: 1.2rem;
            flex-direction: column;
            padding: 20px;
            text-align: center;
        }
        #loadingMessage {
            margin-top: 20px;
            font-size: 1rem;
        }
        
        /* API Key Form */
        #apiKeyForm {
            display: none;
            margin-top: 20px;
            background: rgba(255, 255, 255, 0.1);
            padding: 20px;
            border-radius: 10px;
            width: 90%;
            max-width: 500px;
        }
        #apiKeyInput {
            width: 100%;
            padding: 14px;
            margin: 12px 0;
            border-radius: 8px;
            border: 1px solid #ddd;
            font-size: 16px;
            background: white;
            -webkit-appearance: none;
        }
        #submitApiKey {
            padding: 14px 20px;
            background: #27ae60;
            color: white;
            border: none;
            border-radius: 8px;
            cursor: pointer;
            font-size: 16px;
            font-weight: 600;
        }
        #submitApiKey:active {
            background: #219653;
        }
        .api-key-label {
            font-size: 16px;
            margin-bottom: 12px;
            display: block;
        }
        
        /* OpenAI API Key Form */
        #openaiKeyForm {
            display: none;
            margin-top: 20px;
            background: rgba(255, 255, 255, 0.1);
            padding: 20px;
            border-radius: 10px;
            width: 90%;
            max-width: 500px;
        }
        #openaiKeyInput {
            width: 100%;
            padding: 14px;
            margin: 12px 0;
            border-radius: 8px;
            border: 1px solid #ddd;
            font-size: 16px;
            background: white;
            -webkit-appearance: none;
        }
        #submitOpenaiKey {
            padding: 14px 20px;
            background: #10a37f;
            color: white;
            border: none;
            border-radius: 8px;
            cursor: pointer;
            font-size: 16px;
            font-weight: 600;
        }
        #submitOpenaiKey:active {
            background: #0d8c6c;
        }
        .openai-key-label {
            font-size: 16px;
            margin-bottom: 12px;
            display: block;
        }
        .openai-key-info {
            font-size: 13px;
            color: #aaa;
            margin-top: 8px;
        }
        
        /* Performance warning */
        .performance-warning {
            background: #f39c12;
            color: white;
            padding: 10px;
            border-radius: 8px;
            margin-bottom: 15px;
            text-align: center;
            font-size: 14px;
            display: none;
        }
        
        /* Transformers.js loading indicator */
        #transformersLoading {
            display: none;
            position: fixed;
            top: 50%;
            left: 50%;
            transform: translate(-50%, -50%);
            background: rgba(0, 0, 0, 0.9);
            color: white;
            padding: 20px;
            border-radius: 10px;
            z-index: 1001;
            text-align: center;
        }
        #transformersProgress {
            margin-top: 10px;
            font-size: 14px;
        }
        .progress-bar {
            width: 100%;
            height: 6px;
            background: #333;
            border-radius: 3px;
            margin-top: 10px;
            overflow: hidden;
        }
        .progress-fill {
            height: 100%;
            background: #3498db;
            width: 0%;
            transition: width 0.3s ease;
        }
        
        /* Responsive adjustments */
        @media (max-width: 480px) {
            #outputBox {
                height: 50vh;
                max-height: 50vh;
            }
        }
        
        /* Animation optimizations */
        .typing-cursor {
            display: inline-block;
            background-color: #0f0;
            width: 6px;
            height: 14px;
            animation: blink 1s infinite;
            margin-left: 2px;
            vertical-align: middle;
        }
        @keyframes blink {
            0%, 100% { opacity: 1; }
            50% { opacity: 0; }
        }
        
        /* Simplified dot animation for mobile */
        .dot-flashing {
            display: inline-block;
            width: 8px;
            height: 8px;
            border-radius: 4px;
            background-color: #FFA500;
            animation: dotFlashing 1s infinite linear alternate;
            margin-left: 4px;
        }
        @keyframes dotFlashing {
            0% { opacity: 1; }
            100% { opacity: 0.3; }
        }
    </style>
</head>
<body>
    <div class="performance-warning" id="perfWarning">
        âš ï¸ ç§»åŠ¨è®¾å¤‡æ€§èƒ½æœ‰é™ï¼ŒåŠ è½½å¯èƒ½éœ€è¦æ›´é•¿æ—¶é—´
    </div>
    
    <div class="container">
        <h2>Warsoncoå®¢æœæœºå™¨äºº</h2>
        
        <div class="input-group">
            <input id="inputBox" type="text" placeholder="è¾“å…¥ä½ çš„é—®é¢˜..." disabled />
            <button id="sendButton" disabled>å‘é€</button>
        </div>

        <div id="outputBox"></div>
    </div>

    <div id="loadingOverlay">
        <div>æ­£åœ¨åŠ è½½AIåŠ©æ‰‹...</div>
        <div id="loadingMessage">ğŸ¤– æ­£åœ¨åˆå§‹åŒ–æ™ºèƒ½ä½“...</div>
        
        <!-- API Key Input Form -->
        <div id="apiKeyForm">
            <span class="api-key-label">ğŸ”‘ è¯·è¾“å…¥æ‚¨çš„DeepSeek APIå¯†é’¥:</span>
            <input type="password" id="apiKeyInput" placeholder="sk-xxxxxxxxxxxxxxxx" />
            <button id="submitApiKey">æäº¤</button>
        </div>
        
        <!-- OpenAI API Key Input Form -->
        <div id="openaiKeyForm">
            <span class="openai-key-label">ğŸ”‘ éœ€è¦OpenAI APIå¯†é’¥ç”¨äºæ•°æ®æ£€ç´¢:</span>
            <input type="password" id="openaiKeyInput" placeholder="sk-xxxxxxxxxxxxxxxx" />
            <button id="submitOpenaiKey">æäº¤</button>
            <div class="openai-key-info">Transformers.jsåŠ è½½å¤±è´¥ï¼Œéœ€è¦ä½¿ç”¨OpenAIçš„åµŒå…¥APIä½œä¸ºå¤‡ç”¨æ–¹æ¡ˆ</div>
        </div>
    </div>

    <!-- Transformers.js loading indicator -->
    <div id="transformersLoading">
        <div>åŠ è½½Transformers.jsæ¨¡å‹...</div>
        <div id="transformersProgress">0%</div>
        <div class="progress-bar">
            <div class="progress-fill" id="progressFill"></div>
        </div>
    </div>

    <script type="module">
        // Save OpenAI key globally for checking
        window.openaiKeyAvailable = false;
        window.needOpenaiKey = false;
        
        // Use a lighter model for mobile devices with quantization
        const isMobile = /Android|webOS|iPhone|iPad|iPod|BlackBerry|IEMobile|Opera Mini/i.test(navigator.userAgent);
        const modelName = 'Xenova/all-MiniLM-L6-v2'; // Lightweight model for mobile
        
        // Show loading indicator for Transformers.js
        document.getElementById('transformersLoading').style.display = 'block';
        
        import { pipeline, env } from 'https://gcore.jsdelivr.net/npm/@xenova/transformers@latest';
        
        // Configure environment for mobile optimization
        env.allowRemoteModels = true;
        env.backends.onnx.wasm.numThreads = isMobile ? 1 : 2; // Reduce threads on mobile
        env.backends.onnx.wasm.proxy = true;
        
        // Update progress indicator
        let lastProgressUpdate = 0;
        const progressCallback = (data) => {
            if (data.status === 'progress') {
                const now = Date.now();
                // Throttle progress updates to avoid UI jank
                if (now - lastProgressUpdate > 200) {
                    const percent = Math.round(data.progress * 100);
                    document.getElementById('transformersProgress').textContent = `${percent}%`;
                    document.getElementById('progressFill').style.width = `${percent}%`;
                    lastProgressUpdate = now;
                }
            }
        };
        
        window.embeddingGenerator = null;

        // async function checkNetwork() {
        //     try {
        //         const testResponse = await fetch('https://api.deepseek.com', { method: 'HEAD', timeout: 5000 });
        //         if (!testResponse.ok) throw new Error('Network test failed');
        //         console.log('Network to DeepSeek API is healthy');
        //         return true;
        //     } catch (error) {
        //         console.error('Network check failed:', error);
        //         return false;
        //     }
        // }

        async function initTransformers() {
            try {
                console.log('Loading Transformers.js pipeline with mobile optimization...');
                
                // Use a simpler configuration for mobile with quantization
                const config = {
                    quantized: true, // Use quantized model for mobile
                    progress_callback: progressCallback,
                    device: isMobile ? "wasm" : "webgpu", // Use WASM on mobile for stability
                    dtype: {
                        encoder_model: isMobile ? "q4" : "fp32" // Quantize more on mobile
                    }
                };
                
                window.embeddingGenerator = await pipeline('feature-extraction', modelName, config);
                console.log("Transformers.js pipeline created successfully");
                
                // Hide loading indicator
                document.getElementById('transformersLoading').style.display = 'none';
                
                return true;
            } catch (error) {
                console.error("Failed to initialize Transformers.js:", error);
                // Hide loading indicator even on error
                document.getElementById('transformersLoading').style.display = 'none';
                return false;
            }
        }

        // Initialize Transformers.js with a timeout to prevent hanging 
        const transformersInitPromise = initTransformers();
        
        // Set a timeout to prevent the initialization from hanging indefinitely
        const timeoutPromise = new Promise((resolve) => {
            setTimeout(() => {
                // Show error message but continue
                document.getElementById('transformersLoading').style.display = 'none';
                const loadingMessage = document.getElementById('loadingMessage');
                if (loadingMessage) {
                    loadingMessage.innerHTML = 'Transformers.jsåŠ è½½è¶…æ—¶ï¼Œå°†å°è¯•ä½¿ç”¨å¤‡ç”¨æ¨¡å¼ <span class="dot-flashing"></span>';
                }
                resolve(false);
            }, isMobile ? 90000 : 45000); // 90 seconds timeout on mobile, 45 on desktop
        });

        // Race between initialization and timeout
        Promise.race([transformersInitPromise, timeoutPromise]).then(success => {
            window.transformersInitialized = success;
            if (success) {
                console.log("Transformers.js initialized successfully");
            } else {
                console.warn("Transformers.js initialization failed or timed out");
                // Check if OpenAI key is available in config
                checkOpenAIKey();
            }
        }).catch(error => {
            console.error("Error in Transformers.js initialization:", error);
            window.transformersInitialized = false;
            document.getElementById('transformersLoading').style.display = 'none';
            // Check if OpenAI key is available
            checkOpenAIKey();
        });
        
        function checkOpenAIKey() {
            // Wait for config to load
            if (!window.APP_CONFIG) {
                setTimeout(checkOpenAIKey, 100);
                return;
            }
            
            const openaiKey = window.APP_CONFIG.OPENAI_API_KEY;
            if (openaiKey && openaiKey !== "your_openai_api_key_here") {
                window.openaiKeyAvailable = true;
                console.log("OpenAI API key found in config, will use for embeddings");
            } else {
                window.needOpenaiKey = true;
                console.log("No OpenAI API key found, will prompt user if needed");
            }
        }
    </script>

    <script type="text/javascript">
      // Show performance warning on mobile
      if (/Android|webOS|iPhone|iPad|iPod|BlackBerry|IEMobile|Opera Mini/i.test(navigator.userAgent)) {
        document.getElementById('perfWarning').style.display = 'block';
      }
      
      let pyodide;
      let isStreaming = false;
      let currentStream = null;
      let apiKeyProvided = false;
      let openaiKeyProvided = false;
      let isMobile = /Android|webOS|iPhone|iPad|iPod|BlackBerry|IEMobile|Opera Mini/i.test(navigator.userAgent);
      let lastUserInput = "";
      window.llmTimeout = isMobile ? 180 : 60;
      
      // Function to use OpenAI embeddings
      async function getOpenAIEmbedding(text) {
        const controller = new AbortController();
        const timeoutId = setTimeout(() => controller.abort(), 30000);

        const openaiKey = window.APP_CONFIG.OPENAI_API_KEY;
        if (!openaiKey) {
          throw new Error("No OpenAI API key available");
        }
        
        try {
          const response = await fetch('https://api.openai.com/v1/embeddings', {
            method: 'POST',
            signal: controller.signal,
            headers: {
              'Authorization': `Bearer ${openaiKey}`,
              'Content-Type': 'application/json',
            },
            body: JSON.stringify({
              input: text,
              model: "text-embedding-3-small",
              dimensions: 384
            })
          });
          clearTimeout(timeoutId);
          
          if (!response.ok) {
            throw new Error(`OpenAI API error: ${response.status}`);
          }
          
          const data = await response.json();
          return data.data[0].embedding;
        } catch (error) {
          clearTimeout(timeoutId);
          if (error.name === 'AbortError') {
            throw new Error("OpenAI embedding timeout");
          }
          throw error;
        }
      }
      
      // Function to update status from Python
      function updateStatus(message) {
        showStatus(message + " <span class='dot-flashing'></span>");
        
        // Also update the loading message if overlay is still visible
        const loadingMessage = document.getElementById('loadingMessage');
        if (loadingMessage) {
          loadingMessage.textContent = message;
        }
      }

      // Function to check if API key is provided
      function checkApiKey() {
        // Wait for config to load
        if (!window.APP_CONFIG) {
          setTimeout(checkApiKey, 100);
          return;
        }
        
        const JS_DEEPSEEK_API_KEY = window.APP_CONFIG.DEEPSEEK_API_KEY;
        
        // If no API key in config, show the input form
        if (!JS_DEEPSEEK_API_KEY || JS_DEEPSEEK_API_KEY === "your_deepseek_api_key_here") {
          document.getElementById('apiKeyForm').style.display = 'block';
          document.getElementById('loadingMessage').textContent = 'éœ€è¦DeepSeek APIå¯†é’¥æ‰èƒ½ç»§ç»­';
        } else {
          // API key exists in config, proceed with initialization
          apiKeyProvided = true;
          main();
        }
      }

      // Handle API key submission
      document.getElementById('submitApiKey').addEventListener('click', function() {
        const apiKey = document.getElementById('apiKeyInput').value.trim();
        if (apiKey) {
          // Set the API key in the config
          if (!window.APP_CONFIG) window.APP_CONFIG = {};
          window.APP_CONFIG.DEEPSEEK_API_KEY = apiKey;
          apiKeyProvided = true;
          
          // Hide the form and continue initialization
          document.getElementById('apiKeyForm').style.display = 'none';
          document.getElementById('loadingMessage').textContent = 'ğŸ¤– æ­£åœ¨åˆå§‹åŒ–æ™ºèƒ½ä½“...';
          main();
        } else {
          alert('è¯·è¾“å…¥æœ‰æ•ˆçš„APIå¯†é’¥');
        }
      });

      // Also allow Enter key to submit
      document.getElementById('apiKeyInput').addEventListener('keypress', function(e) {
        if (e.key === 'Enter') {
          document.getElementById('submitApiKey').click();
        }
      });
      
      // Handle OpenAI API key submission
      document.getElementById('submitOpenaiKey').addEventListener('click', async function() {
        const apiKey = document.getElementById('openaiKeyInput').value.trim();
        if (apiKey) {
          // Set the OpenAI API key in the config
          if (!window.APP_CONFIG) window.APP_CONFIG = {};
          window.APP_CONFIG.OPENAI_API_KEY = apiKey;
          window.openaiKeyAvailable = true;
          openaiKeyProvided = true;
          
          await pyodide.runPythonAsync(`
            import os
            os.environ['OPENAI_API_KEY'] = "${apiKey}"
          `);
          
          // Hide the form
          document.getElementById('openaiKeyForm').style.display = 'none';
          
          if (document.getElementById('loadingOverlay').style.display === 'flex' && window.transformersInitialized === false) {
            // Init phase for embeddings
            document.getElementById('loadingMessage').textContent = 'ğŸ¤– æ­£åœ¨åˆå§‹åŒ–æ™ºèƒ½ä½“...';
            continueMainInit();
          } else {
            // Runtime fallback for LLM
            document.getElementById('loadingOverlay').style.display = 'none';
            hideStatus();
            
            await pyodide.runPythonAsync(`
              from langchain_openai import ChatOpenAI
              fallback_llm = ChatOpenAI(model="gpt-4.1-nano", temperature=0, request_timeout=${window.llmTimeout}) # gpt-4o-mini
              agent_instance.fallback_llm = fallback_llm
              print("Fallback LLM set")
            `);
            
            showStatus("ä½¿ç”¨OpenAIé‡è¯• <span class='dot-flashing'></span>");
            
            const retryResult = await pyodide.runPythonAsync(`
              await main_function(${JSON.stringify(lastUserInput)})
            `);
            
            hideStatus();
            
            streamOutput("ğŸ¤– åŠ©æ‰‹: " + retryResult, "assistant");
          }
        } else {
          alert('è¯·è¾“å…¥æœ‰æ•ˆçš„OpenAI APIå¯†é’¥');
        }
      });

      // Also allow Enter key to submit OpenAI key
      document.getElementById('openaiKeyInput').addEventListener('keypress', function(e) {
        if (e.key === 'Enter') {
          document.getElementById('submitOpenaiKey').click();
        }
      });

      async function main(){
        // Show loading overlay initially
        document.getElementById('loadingOverlay').style.display = 'flex';
        
        // Wait for config and API key
        while (!window.APP_CONFIG || !apiKeyProvided) {
          await new Promise(r => setTimeout(r, 50));
        }
        
        const JS_LLM_MODE = window.APP_CONFIG.LLM_MODE;
        const JS_DEEPSEEK_API_KEY = window.APP_CONFIG.DEEPSEEK_API_KEY;
        const JS_HUGGINGFACEHUB_API_TOKEN = window.APP_CONFIG.HUGGINGFACEHUB_API_TOKEN;
        const JS_TAVILY_API_KEY = window.APP_CONFIG.TAVILY_API_KEY;
        
        // Default to transformersjs for mobile if not specified
        let JS_EMBEDDING_MODE = window.APP_CONFIG.EMBEDDING_MODE || (isMobile ? "transformersjs" : "local");

        // Wait for Transformers.js to initialize if we're using it
        if (JS_EMBEDDING_MODE === "transformersjs") {
          updateStatus("ğŸ”„ æ­£åœ¨åŠ è½½Transformers.jsæ¨¡å‹...");
          
          // Show Transformers.js loading indicator
          document.getElementById('transformersLoading').style.display = 'block';
          
          // Wait for Transformers.js to initialize or timeout
          let waitTime = 0;
          while (window.transformersInitialized === undefined && waitTime < 90000) {
            await new Promise(r => setTimeout(r, 100));
            waitTime += 100;
          }
          
          // Hide Transformers.js loading indicator
          document.getElementById('transformersLoading').style.display = 'none';
          
          if (!window.transformersInitialized) {
            console.warn("Transformers.js failed to initialize, checking for OpenAI fallback");
            
            // Check if OpenAI key is available
            if (window.openaiKeyAvailable) {
              JS_EMBEDDING_MODE = "openai";
              window.APP_CONFIG.EMBEDDING_MODE = "openai";
              console.log("Using OpenAI embeddings as fallback");
            } else if (window.needOpenaiKey) {
              // Need to prompt for OpenAI key
              document.getElementById('openaiKeyForm').style.display = 'block';
              document.getElementById('loadingMessage').textContent = 'éœ€è¦OpenAI APIå¯†é’¥ç”¨äºåµŒå…¥å‘é‡';
              
              // Wait for OpenAI key to be provided
              while (!openaiKeyProvided) {
                await new Promise(r => setTimeout(r, 100));
              }
              
              JS_EMBEDDING_MODE = "openai";
              window.APP_CONFIG.EMBEDDING_MODE = "openai";
            } else {
              // Fall back to local mode
              JS_EMBEDDING_MODE = "local";
              window.APP_CONFIG.EMBEDDING_MODE = "local";
              console.log("Using local embeddings as fallback");
            }
          }
        }
        
        // Continue with initialization
        await continueMainInit();
      }
      
      async function continueMainInit() {
        const JS_LLM_MODE = window.APP_CONFIG.LLM_MODE;
        const JS_DEEPSEEK_API_KEY = window.APP_CONFIG.DEEPSEEK_API_KEY;
        const JS_HUGGINGFACEHUB_API_TOKEN = window.APP_CONFIG.HUGGINGFACEHUB_API_TOKEN;
        const JS_TAVILY_API_KEY = window.APP_CONFIG.TAVILY_API_KEY;
        const JS_OPENAI_API_KEY = window.APP_CONFIG.OPENAI_API_KEY || "";
        const JS_EMBEDDING_MODE = window.APP_CONFIG.EMBEDDING_MODE;

        // Set dynamic timeout based on device
        const llmTimeout = isMobile ? 180 : 60;  // 3min on mobile, 1min on PC
        window.APP_CONFIG.LLM_TIMEOUT = llmTimeout;
        
        // Initialize Pyodide with a progress callback
        updateStatus("ğŸ”„ æ­£åœ¨åŠ è½½Pyodide...");
        pyodide = await loadPyodide({
          // Show loading progress for better UX
          stdout: msg => console.log(msg),
          stderr: msg => console.error(msg)
        });
        
        // Register the updateStatus function in Python's global scope
        pyodide.globals.set("updateStatus", updateStatus);
        // Pass embedding mode to Python
        pyodide.globals.set("JS_EMBEDDING_MODE", JS_EMBEDDING_MODE);

        // --- 1. Fetch embeddings.npy with progress tracking ---
        updateStatus("ğŸ“¥ æ­£åœ¨åŠ è½½åµŒå…¥æ•°æ®...");
        try {
          const npyResp = await fetchWithProgress("https://mofanv.github.io/dist/embeddings.npy", 
            "embeddings.npy", "åµŒå…¥æ•°æ®");
          if (!npyResp.ok) throw new Error("åŠ è½½RAG Embeddingæ•°æ®æ–‡ä»¶å¤±è´¥ï¼Œè¯·æŸ¥çœ‹æ˜¯å¦æœ‰è®¿é—®æƒé™ã€‚");
          const npyBuffer = await npyResp.arrayBuffer();
          pyodide.FS.writeFile("/embeddings.npy", new Uint8Array(npyBuffer));
        } catch (error) {
          console.error("Error loading embeddings:", error);
          updateStatus("âŒ åŠ è½½åµŒå…¥æ•°æ®å¤±è´¥");
          // Continue with a fallback - create empty embeddings
          const fallbackEmbeddings = new Float32Array(384); // Default size
          pyodide.FS.writeFile("/embeddings.npy", fallbackEmbeddings);
        }

        // --- 2. Fetch embeddings_docs.json with progress tracking ---
        updateStatus("ğŸ“¥ æ­£åœ¨åŠ è½½æ–‡æ¡£æ•°æ®...");
        try {
          const jsonResp = await fetchWithProgress("https://mofanv.github.io/dist/embeddings_docs.json", 
            "embeddings_docs.json", "æ–‡æ¡£æ•°æ®");
          if (!jsonResp.ok) throw new Error("åŠ è½½RAG Docsæ•°æ®æ–‡ä»¶å¤±è´¥ï¼Œè¯·æŸ¥çœ‹æ˜¯å¦æœ‰è®¿é—®æƒé™ã€‚");
          const jsonText = await jsonResp.text();
          pyodide.FS.writeFile("/embeddings_docs.json", jsonText);
        } catch (error) {
          console.error("Error loading docs:", error);
          updateStatus("âŒ åŠ è½½æ–‡æ¡£æ•°æ®å¤±è´¥");
          // Continue with a fallback - create empty docs
          const fallbackDocs = JSON.stringify([{text: "No data available", metadata: {url: "N/A"}}]);
          pyodide.FS.writeFile("/embeddings_docs.json", fallbackDocs);
        }

        updateStatus("ğŸ“¦ æ­£åœ¨å®‰è£…PythonåŒ…...");
        await pyodide.loadPackage("micropip");

        // Install only essential packages for mobile 
        const packagesToInstall = ["requests==2.32.5", "numpy", "langchain", "langchain_deepseek", "langchain_openai"];

        console.log(await pyodide.runPythonAsync(`
            import sys
            sys.version

            import micropip

            await micropip.install(${JSON.stringify(packagesToInstall)}, keep_going=True)
        `));

        await pyodide.runPythonAsync(`
            import os
            os.environ['DEEPSEEK_API_KEY'] = "${JS_DEEPSEEK_API_KEY}"
            os.environ['HUGGINGFACEHUB_API_TOKEN'] = "${JS_HUGGINGFACEHUB_API_TOKEN}"
            os.environ['OPENAI_API_KEY'] = "${JS_OPENAI_API_KEY}"
            os.environ['LLM_MODE'] = "${JS_LLM_MODE}"
            os.environ['EMBEDDING_MODE'] = "${JS_EMBEDDING_MODE}"
            print("API Key set:", os.environ['DEEPSEEK_API_KEY'][:6] + "...")
            print("LLM Mode:", os.environ['LLM_MODE'])
            print("Embedding Mode:", os.environ['EMBEDDING_MODE'])
            if os.environ.get('OPENAI_API_KEY'):
                print("OpenAI Key available:", os.environ['OPENAI_API_KEY'][:6] + "...")
        `);
        
        // Now run the main Python code
        updateStatus("ğŸ æ­£åœ¨æ‰§è¡ŒPythonä»£ç ...");
        
        // Use a simplified Python code for mobile to improve performance 
        const pythonCode = getPythonCode(JS_EMBEDDING_MODE);
        
        await pyodide.runPythonAsync(pythonCode);
        
        // Hide the loading overlay and enable input
        document.getElementById('loadingOverlay').style.display = 'none';
        document.getElementById('inputBox').disabled = false;
        document.querySelector('button').disabled = false;
        document.getElementById('inputBox').focus();
        
        // Clear any status messages in the chat output
        hideStatus();
      }

        // Function to fetch with progress tracking
        async function fetchWithProgress(url, filename, description) {
        const response = await fetch(url);
        const reader = response.body.getReader();
        const chunks = [];
        let loaded = 0;
        let startTime = Date.now();
        let lastUpdateTime = startTime;
        let lastLoaded = 0;

        while(true) {
            const {done, value} = await reader.read();
            if (done) break;
            
            chunks.push(value);
            loaded += value.length;
            
            const currentTime = Date.now();
            const timeElapsed = (currentTime - lastUpdateTime) / 1000; // in seconds
            
            // Update every 500ms or when 1MB is downloaded
            if (timeElapsed > 0.5 || loaded - lastLoaded >= 1024 * 1024) {
            const downloadSpeed = (loaded - lastLoaded) / timeElapsed; // bytes per second
            const speedText = formatBytes(downloadSpeed) + '/s';
            
            updateStatus(`ğŸ“¥ æ­£åœ¨åŠ è½½${description}... ${formatBytes(loaded)} (${speedText})`);
            
            lastUpdateTime = currentTime;
            lastLoaded = loaded;
            }
        }
        
        // Final download complete message
        const totalTime = (Date.now() - startTime) / 1000;
        const averageSpeed = loaded / totalTime;
        const speedText = formatBytes(averageSpeed) + '/s';
        updateStatus(`âœ… ${description}åŠ è½½å®Œæˆ! ${formatBytes(loaded)} (å¹³å‡é€Ÿåº¦: ${speedText})`);

        // Combine chunks
        const blob = new Blob(chunks);
        return new Response(blob);
        }

        // Helper function to format bytes to human-readable format
        function formatBytes(bytes, decimals = 2) {
        if (bytes === 0) return '0 B';
        
        const k = 1024;
        const sizes = ['B', 'KB', 'MB', 'GB'];
        const i = Math.floor(Math.log(bytes) / Math.log(k));
        
        return parseFloat((bytes / Math.pow(k, i)).toFixed(decimals)) + ' ' + sizes[i];
        }
      
      // Get appropriate Python code based on embedding mode
      function getPythonCode(embeddingMode) {
        // Mobile-optimized Python code with better error handling
        return `
import json
import os
import numpy as np
from langchain.schema import Document
from langchain.tools import tool
from langchain.memory import ConversationBufferMemory
from langchain.agents import initialize_agent, AgentType
from langchain_deepseek import ChatDeepSeek
from langchain_openai import ChatOpenAI

# ===================== LLM SELECTION =====================
LLM_MODE = os.getenv("LLM_MODE", "deepseek")
EMBEDDING_MODE = os.getenv("EMBEDDING_MODE", "local")
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY", "")

timeout_val = int(os.getenv("LLM_TIMEOUT", "60"))  # Defaults to 60 if not set

if LLM_MODE == "deepseek":
    llm = ChatDeepSeek(model="deepseek-chat", timeout=timeout_val)
    print(f"Using DeepSeek with timeout: {timeout_val}s")
    print("Using DeepSeek as the Agent LLM...")
else:
    raise ValueError(f"Unsupported LLM_MODE: {LLM_MODE}")

fallback_llm = None
if OPENAI_API_KEY:
    fallback_llm = ChatOpenAI(model="gpt-3.5-turbo", temperature=0, request_timeout=timeout_val)
    print("OpenAI fallback LLM initialized")

print(f"Using embedding mode: {EMBEDDING_MODE}")

use_openai = False

# ===================== KNOWLEDGE BASE =====================
EMBEDDINGS_FILE = "/embeddings.npy"
DOCS_FILE = "/embeddings_docs.json"

# Load precomputed embeddings
try:
    vectors = np.load(EMBEDDINGS_FILE, allow_pickle=True)
    vectors = np.array(vectors, dtype=np.float32)
    # Normalize vectors
    norms = np.linalg.norm(vectors, axis=1, keepdims=True)
    vectors /= (norms + 1e-10)
except:
    vectors = np.zeros((1, 384), dtype=np.float32)  # Fallback

# Load metadata/docs
try:
    with open(DOCS_FILE, "r", encoding="utf-8") as f:
        docs_store = [Document(page_content=d["text"], metadata=d["metadata"]) for d in json.load(f)]
except:
    docs_store = [Document(page_content="No data available", metadata={})]  # Fallback




def get_embedding(text: str):
    """Get embedding based on mode"""
    updateStatus("ğŸ” æ­£åœ¨ç”ŸæˆåµŒå…¥å‘é‡...")
    try:
        if EMBEDDING_MODE == "transformersjs":
            try:
                # Use Transformers.js embedding from JavaScript
                import numpy as np
                emb = np.array(js_embedding, dtype=np.float32)
                # Normalize
                norm = np.linalg.norm(emb)
                if norm > 0:
                    emb /= norm
                return emb
            except NameError:
                print("[Warning] js_embedding not available, falling back")
                pass  # Fall through to next
        elif EMBEDDING_MODE == "openai":
            try:
                # Use OpenAI embedding from JavaScript
                import numpy as np
                emb = np.array(js_openai_embedding, dtype=np.float32)
                # Normalize
                norm = np.linalg.norm(emb)
                if norm > 0:
                    emb /= norm
                return emb
            except NameError:
                print("[Warning] js_openai_embedding not available, falling back")
                pass  # Fall through to next
        
        # Fallback to simple embedding only if above fails (e.g., no JS var set)
        import numpy as np
        # Simple hash-based embedding for mobile fallback
        words = text.lower().split()
        emb = np.zeros(384, dtype=np.float32)
        count = 0
        for word in words:
            if len(word) > 2:  # Skip very short words
                seed = hash(word) % 1000
                np.random.seed(seed)
                emb += np.random.rand(384).astype(np.float32) - 0.5
                count += 1
        if count > 0:
            emb /= count
            norm = np.linalg.norm(emb)
            if norm > 0:
                emb /= norm
        return emb
    except Exception as e:
        print(f"[Embedding Error] {e}")
        return np.zeros(vectors.shape[1], dtype=np.float32) if vectors.size > 0 else np.zeros(384, dtype=np.float32)

        
    
def query_kb(query: str, k: int = 3):
    """Query the knowledge base"""
    updateStatus("ğŸ“š æ­£åœ¨æœç´¢çŸ¥è¯†åº“...")
    q_vec = get_embedding(query)
    
    # Check if vectors are available
    if vectors.size == 0 or vectors.shape[1] != q_vec.shape[0]:
        updateStatus("âš ï¸ çŸ¥è¯†åº“æœªåŠ è½½ï¼Œä½¿ç”¨é»˜è®¤å›å¤")
        return [Document(page_content="çŸ¥è¯†åº“æš‚ä¸å¯ç”¨ï¼Œè¯·ç¨åå†è¯•", metadata={"url": "N/A"})]
    
    try:
        sims = np.dot(q_vec, vectors.T)[0]
        top_idx = sims.argsort()[-k:][::-1]
        updateStatus("âœ… å·²æ‰¾åˆ°ç›¸å…³æ–‡æ¡£")
        return [docs_store[i] for i in top_idx]
    except:
        # Return empty results on error
        return []

# ===================== TOOLS =====================
@tool
def search_knowledge(query: str):
    """æ£€ç´¢çŸ¥è¯†åº“ç›¸å…³ä¿¡æ¯"""
    updateStatus("ğŸ” æ­£åœ¨æœç´¢çŸ¥è¯†åº“...")
    docs = query_kb(query, k=3)
    return [f"{d.page_content}\\næ¥æº: {d.metadata.get('url','æœªçŸ¥æ¥æº')}" for d in docs]

@tool
def get_price_info(query: str):
    """æ£€ç´¢ä»·æ ¼ç›¸å…³ä¿¡æ¯"""
    updateStatus("ğŸ’° æ­£åœ¨æœç´¢ä»·æ ¼ä¿¡æ¯...")
    docs = query_kb(query, k=3)
    price_docs = [d for d in docs if "ä»·" in d.page_content or "ï¿¥" in d.page_content]
    return [f"{d.page_content}\\næ¥æº: {d.metadata.get('url','æœªçŸ¥æ¥æº')}" for d in price_docs] if price_docs else "æœªæ‰¾åˆ°ç›¸å…³ä»·æ ¼ä¿¡æ¯"

# ===================== AGENT SETUP =====================
tools = [search_knowledge, get_price_info]

system_prompt = """ä½ æ˜¯ã€åç››æ§ç§‘æŠ€æ™ºèƒ½å®¢æœåŠ©æ‰‹ã€‘ï¼ŒèŒè´£æ˜¯ä¸ºæ½œåœ¨å®¢æˆ·å’Œåˆä½œä¼™ä¼´æä¾›ä¸“ä¸šã€ç®€æ´ã€å¯é çš„æ”¯æŒã€‚

### ä½ çš„èƒ½åŠ›ä¸è¾¹ç•Œ
1. **ä¿¡æ¯æ¥æº**  
   - ä¼˜å…ˆä½¿ç”¨çŸ¥è¯†åº“ï¼ˆå®˜ç½‘å†…å®¹ã€äº§å“æ‰‹å†Œã€å¸¸è§é—®é¢˜ã€å”®åæ”¿ç­–ç­‰ï¼‰å›ç­”ã€‚  
   - å¦‚æœçŸ¥è¯†åº“æ²¡æœ‰ç›¸å…³ä¿¡æ¯ï¼Œè¯·å¦è¯šè¯´æ˜ï¼Œå¹¶ç¤¼è²Œå¼•å¯¼ç”¨æˆ·è”ç³»äººå·¥å®¢æœã€‚  

2. **å›ç­”è§„åˆ™**  
   - å›ç­”è¦æ¸…æ™°ã€ç®€æ´ã€åˆ†ç‚¹åˆ—å‡ºï¼Œä¾¿äºå®¢æˆ·å¿«é€Ÿç†è§£ã€‚  
   - ä»…æä¾›ä¸å…¬å¸ã€äº§å“ã€æœåŠ¡ç›¸å…³çš„å†…å®¹ï¼Œä¸å›ç­”æ— å…³è¯é¢˜ã€‚  
   - å›ç­”æ—¶é¿å…å†—é•¿å’Œç©ºè¯ã€‚  

3. **äº¤äº’é£æ ¼**  
   - ä¿æŒä¸“ä¸šã€å‹å¥½ã€è€å¿ƒã€‚  
   - å¦‚æœé—®é¢˜æ¨¡ç³Šï¼Œè¯·ä¸»åŠ¨è¿½é—®æ¾„æ¸…ã€‚  
   - ä¼˜å…ˆä½¿ç”¨å®¢æˆ·è¾“å…¥çš„è¯­è¨€å›ç­”ï¼ˆä¸­æ–‡æˆ–è‹±æ–‡ï¼‰ã€‚  

4. **åœºæ™¯å¼•å¯¼**  
   - **äº§å“å’¨è¯¢**ï¼šå¯æ¨èäº§å“ã€åˆ—å‡ºå‚æ•°å¯¹æ¯”ï¼Œå¹¶å¼•å¯¼ç”¨æˆ·ä¸‹è½½èµ„æ–™æˆ–è”ç³»é”€å”®ã€‚  
   - **å”®åæ”¯æŒ**ï¼šæä¾›æ’æŸ¥å»ºè®®ï¼›å¦‚æœè¶…å‡ºèƒ½åŠ›èŒƒå›´ï¼Œæç¤ºæŠ¥ä¿®æˆ–è”ç³»å”®åã€‚  
   - **æ–¹æ¡ˆ/æŠ¥ä»·éœ€æ±‚**ï¼šæ”¶é›†éœ€æ±‚ä¿¡æ¯ï¼ˆè¡Œä¸šã€äº§èƒ½ã€è”ç³»æ–¹å¼ç­‰ï¼‰ï¼Œå¹¶å»ºè®®åç»­äººå·¥å¯¹æ¥ã€‚  
   - **è”ç³»æ–¹å¼**ï¼šå¯æä¾›å…¬å¸ç”µè¯ã€é‚®ç®±ã€åœ¨çº¿å®¢æœå…¥å£ç­‰ã€‚  

5. **é¢å¤–è¦æ±‚**  
   - åœ¨æ¶‰åŠè´­ä¹°ã€æŠ¥ä»·ã€æ–¹æ¡ˆå®šåˆ¶æ—¶ï¼Œè¦å¼•å¯¼ç”¨æˆ·ç•™ä¸‹è”ç³»æ–¹å¼ã€‚  
   - é‡åˆ°é‡å¤é—®é¢˜æ—¶ï¼Œä¿æŒè€å¿ƒå¹¶å°è¯•æ¢ç§è¡¨è¿°æ–¹å¼ã€‚  
"""

memory = ConversationBufferMemory(
    memory_key="chat_history",
    return_messages=True,
)

updateStatus("ğŸ¤– æ­£åœ¨åˆå§‹åŒ–æ™ºèƒ½ä½“...")
is_mobile = globals().get("JS_IS_MOBILE", False)
agent = initialize_agent(
    tools,
    llm,
    agent=AgentType.CHAT_CONVERSATIONAL_REACT_DESCRIPTION,
    memory=memory,
    verbose=False,  # Reduced verbosity for mobile
    max_iterations=1 if is_mobile else 2,  # Reduce on mobile
    early_stopping_method="generate"
)

# ===================== AGENT INTERFACE =====================
class ProductAssistantAgent:
    def __init__(self, agent_llm, fallback_llm=None):
        self.agent = agent_llm
        self.fallback_llm = fallback_llm

    def is_input_relevant(self, user_input: str, threshold=0.2):
        updateStatus("ğŸ” æ­£åœ¨æ£€æŸ¥æŸ¥è¯¢ç›¸å…³æ€§...")
        q_vec = get_embedding(user_input)
        
        if vectors.size == 0:
            print("[Debug] Vectors empty, assuming relevant")
            return True
            
        vecs = vectors.astype(np.float32)
        print(f"[Debug] q_vec shape: {q_vec.shape}, vecs shape: {vecs.shape}")
        
        if q_vec.shape[0] != vecs.shape[1]:
            print("[Debug] Shape mismatch!")
            return False

        try:
            updateStatus("ğŸ“Š æ­£åœ¨è®¡ç®—ç›¸ä¼¼åº¦åˆ†æ•°...")
            sims = np.dot(q_vec, vecs.T)
            print(f"[Debug] sims shape: {sims.shape}, max_sim: {np.max(sims)}")
            if sims.size == 0:
                return False
            max_sim = float(np.max(sims))
            print(f"[Debug] max_sim {max_sim} >= {threshold}: {max_sim >= float(threshold)}")
            return max_sim >= float(threshold)
        except (TypeError, ValueError) as e:
            print(f"[Debug] Sim error: {e}")
            return False


    async def process_message(self, message: str):
        global use_openai
        if not self.is_input_relevant(message):
            return "æŠ±æ­‰ï¼Œæˆ‘ä¸å¤ªç†è§£æ‚¨çš„é—®é¢˜ã€‚å¯ä»¥è¯·æ‚¨æè¿°å¾—æ›´æ¸…æ¥šå—ï¼Ÿ"

        updateStatus("ğŸ§  æ­£åœ¨æ€è€ƒå¦‚ä½•å›ç­”...")
        if use_openai and self.fallback_llm:
            updateStatus("ğŸ§  ä½¿ç”¨ OpenAI æ€è€ƒ...")
            fallback_agent = initialize_agent(
                tools,
                self.fallback_llm,
                agent=AgentType.CHAT_CONVERSATIONAL_REACT_DESCRIPTION,
                memory=memory,
                verbose=False,
                max_iterations=1 if is_mobile else 2,
                early_stopping_method="generate"
            )
            try:
                ai_raw = fallback_agent.invoke({
                    "input": f"""{system_prompt}\n\nç”¨æˆ·é—®é¢˜: {message}""",
                    "chat_history": memory.load_memory_variables({})["chat_history"]
                })
                success = True
            except Exception as fallback_e:
                print(f"[Fallback Error] {fallback_e}")
                return "å¤‡ç”¨ OpenAI API ä¹Ÿå‡ºç°é—®é¢˜ï¼Œè¯·æ£€æŸ¥ç½‘ç»œå¹¶ç¨åé‡è¯•ã€‚"
        else:
            max_retries = 2
            success = False
            ai_raw = None
            for attempt in range(max_retries):
                try:
                    # Add explicit timeout to llm call if possible
                    ai_raw = self.agent.invoke({
                        "input": f"""{system_prompt}\n\nç”¨æˆ·é—®é¢˜: {message}""",
                        "chat_history": memory.load_memory_variables({})["chat_history"]
                    })
                    success = True
                    break
                except Exception as e:
                    error_str = str(e).lower()
                    if "timeout" in error_str or "connection" in error_str:
                        if attempt < max_retries - 1:
                            wait_time = 2 ** attempt
                            print(f"[Retry {attempt+1}/{max_retries}] Timeout: {e}. Waiting {wait_time}s...")
                            import asyncio
                            await asyncio.sleep(wait_time)
                            continue
                    elif "invalid response" in error_str or "json" in error_str:
                        if attempt < max_retries - 1:
                            wait_time = 2 ** attempt
                            print(f"[Retry {attempt+1}/{max_retries}] Invalid response. Waiting {wait_time}s...")
                            import asyncio
                            await asyncio.sleep(wait_time)
                            continue
                        else:
                            break  # Will trigger fallback below
                    else:
                        raise e

            if not success:
                use_openai = True
                if self.fallback_llm:
                    updateStatus("âš ï¸ DeepSeek API å¤±è´¥ï¼Œåˆ‡æ¢åˆ° OpenAI...")
                    fallback_agent = initialize_agent(
                        tools,
                        self.fallback_llm,
                        agent=AgentType.CHAT_CONVERSATIONAL_REACT_DESCRIPTION,
                        memory=memory,
                        verbose=False,
                        max_iterations=1 if is_mobile else 2,
                        early_stopping_method="generate"
                    )
                    try:
                        ai_raw = fallback_agent.invoke({
                            "input": f"""{system_prompt}\n\nç”¨æˆ·é—®é¢˜: {message}""",
                            "chat_history": memory.load_memory_variables({})["chat_history"]
                        })
                        success = True
                    except Exception as fallback_e:
                        print(f"[Fallback Error] {fallback_e}")
                        return "å¤‡ç”¨ OpenAI API ä¹Ÿå‡ºç°é—®é¢˜ï¼Œè¯·æ£€æŸ¥ç½‘ç»œå¹¶ç¨åé‡è¯•ã€‚"
                else:
                    return "NEED_OPENAI_KEY"

        if not success:
            return "APIå“åº”æ— æ•ˆï¼Œè¯·æ£€æŸ¥ç½‘ç»œæˆ–APIå¯†é’¥ã€‚å»ºè®®ä½¿ç”¨Wi-Fié‡è¯•ã€‚"

        # Extract response
        if hasattr(ai_raw, "content"):
            ai_response = ai_raw.content
        elif isinstance(ai_raw, dict) and "output" in ai_raw:
            ai_response = ai_raw["output"]
        else:
            ai_response = str(ai_raw)

        # Handle errors in response content
        if "invalid response" in ai_response.lower() or "api" in ai_response.lower():
            return "ç½‘ç»œè¿æ¥å‡ºç°é—®é¢˜ï¼Œè¯·ç¨åé‡è¯•æˆ–æ£€æŸ¥APIå¯†é’¥æ˜¯å¦æ­£ç¡®ã€‚"
            
        return ai_response



# ===================== GLOBAL AGENT INSTANCE =====================
agent_instance = ProductAssistantAgent(agent_llm=agent, fallback_llm=fallback_llm)

# ===================== MAIN FUNCTION =====================
async def main_function(user_input: str) -> str:
    updateStatus("ğŸ¤– æ­£åœ¨å¤„ç†æ‚¨çš„è¯·æ±‚...")
    res = await agent_instance.process_message(user_input)
    return res
          `;
      }

        // Attach click event listener for better mobile compatibility
        document.getElementById('sendButton').addEventListener('click', function(event) {
            event.preventDefault();  // Prevent any default touch behavior
            sendToPyodide();
        });

        // Also ensure Enter key works (already there, but confirm)
        document.getElementById("inputBox").addEventListener("keydown", function(event) {
            if (event.key === 'Enter') {
                event.preventDefault();
                sendToPyodide();
            }
        });
      
      // Start checking for API key when page loads
      checkApiKey();

    async function sendToPyodide() {
        
        if (isStreaming) {
            // If already streaming, cancel the current stream
            if (currentStream) {
            clearInterval(currentStream);
            currentStream = null;
            }
            // Remove the cursor
            const cursor = document.querySelector('.typing-cursor');
            if (cursor) cursor.remove();
            isStreaming = false;
        }
        
        const userInput = document.getElementById("inputBox").value;
        if (!userInput.trim()) {
            return;
        }
        
        lastUserInput = userInput;
        
        logOutput("ğŸ‘¤ ç”¨æˆ·: " + userInput, "user");
        
        // Disable input during processing
        document.getElementById("inputBox").value = "";
        document.getElementById("inputBox").disabled = true;
        document.querySelector('button').disabled = true;
        
        // Show initial status
        showStatus("ğŸ¤– æ­£åœ¨å¤„ç†æ‚¨çš„è¯·æ±‚ <span class='dot-flashing'></span>");
        
        try {
            
            // Check which embedding mode to use
            let embeddingMode = window.APP_CONFIG.EMBEDDING_MODE || "local";

            if (embeddingMode === "transformersjs" && window.embeddingGenerator) {
            // Use Transformers.js to generate embeddings
            showStatus("ğŸ§  æ­£åœ¨ä½¿ç”¨ Transformers.js ç”ŸæˆåµŒå…¥ <span class='dot-flashing'></span>");
            
            // Generate embedding vector
            const output = await window.embeddingGenerator(userInput, { pooling: 'mean', normalize: true });
            const embeddingArray = Array.from(output.data);
            
            // Pass embedding vector to Pyodide
            pyodide.globals.set("js_embedding", embeddingArray);

            } else if (embeddingMode === "openai") {
            // Use OpenAI to generate embeddings
            showStatus("ğŸ§  æ­£åœ¨ä½¿ç”¨ OpenAI API ç”ŸæˆåµŒå…¥ <span class='dot-flashing'></span>");
            
            try {
                const embeddingArray = await getOpenAIEmbedding(userInput);
                // Pass embedding vector to Pyodide
                pyodide.globals.set("js_openai_embedding", embeddingArray);
            } catch (error) {
                console.error("Failed to get OpenAI embedding:", error);
                // Fall back to local mode
                showStatus("âš ï¸ OpenAIåµŒå…¥å¤±è´¥ï¼Œä½¿ç”¨æœ¬åœ°æ¨¡å¼");
            }
            }
            
            // const networkOk = await checkNetwork();
            // if (!networkOk) {
            //     hideStatus();
            //     logOutput("âš ï¸ ç½‘ç»œè¿æ¥ä¸ç¨³å®šï¼Œè¯·è¿æ¥Wi-Fiå¹¶é‡è¯•ã€‚", "error");
            //     document.getElementById("inputBox").disabled = false;
            //     document.querySelector('button').disabled = false;
            //     return;
            // }
            
            // Process user input
            const result = await pyodide.runPythonAsync(`
                await main_function(${JSON.stringify(userInput)})
            `);
            
            if (result === "NEED_OPENAI_KEY") {
              document.getElementById('openaiKeyForm').style.display = 'block';
              document.getElementById('loadingMessage').textContent = 'DeepSeek API å¤±æ•ˆï¼Œéœ€è¦OpenAI APIå¯†é’¥ä½œä¸ºå¤‡ç”¨';
              document.getElementById('loadingOverlay').style.display = 'flex';
              document.getElementById('openaiKeyForm').querySelector('.openai-key-info').textContent = 'DeepSeek API è¿”å›æ— æ•ˆå“åº”ï¼Œéœ€è¦OpenAIä½œä¸ºå¤‡ç”¨';
              return;
            }
            
            // Clear status
            hideStatus();
            
            // Start streaming the response
            streamOutput("ğŸ¤– åŠ©æ‰‹: " + result, "assistant");
        } catch (err) {
            // Clear status
            hideStatus();
            
            logOutput("âš ï¸ é”™è¯¯: " + err, "error");
            // Re-enable input on error
            document.getElementById("inputBox").disabled = false;
            document.querySelector('button').disabled = false;
        }
    
    }

      document.getElementById("inputBox").addEventListener("keydown", function(event) {
          if (event.key === 'Enter') {
              event.preventDefault(); // prevent newline in input
              sendToPyodide();
          }
      });

        function logOutput(message, type="assistant") {
            const out = document.getElementById("outputBox");
            
            // åˆ›å»ºæ¶ˆæ¯å…ƒç´ 
            const span = document.createElement("span");
            span.textContent = message;

            if (type === "user") {
                span.style.color = "#1E90FF";
                span.style.fontWeight = "bold";
            } else if (type === "assistant") {
                span.style.color = "#32CD32";
            } else if (type === "error") {
                span.style.color = "#FF4500";
                span.style.fontWeight = "bold";
            }

            out.appendChild(span);
            out.appendChild(document.createElement("br"));
            out.appendChild(document.createElement("br"));

            // ç¡®ä¿æ»šåŠ¨åˆ°åº•éƒ¨
            out.scrollTop = out.scrollHeight;
        }
      

        function streamOutput(message, type) {
            isStreaming = true;
            const out = document.getElementById("outputBox");
            
            // åˆ›å»ºæ¶ˆæ¯å…ƒç´ 
            const span = document.createElement("span");
            if (type === "assistant") {
                span.style.color = "#32CD32";
            }
            
            out.appendChild(span);
            
            // åˆ›å»ºå…‰æ ‡
            const cursor = document.createElement("span");
            cursor.className = "typing-cursor";
            out.appendChild(cursor);
            
            let i = 0;
            currentStream = setInterval(() => {
                if (i < message.length) {
                    span.textContent += message[i];
                    i++;
                    // æ¯æ¬¡æ·»åŠ å­—ç¬¦æ—¶éƒ½æ»šåŠ¨åˆ°åº•éƒ¨
                    out.scrollTop = out.scrollHeight;
                } else {
                    clearInterval(currentStream);
                    currentStream = null;
                    isStreaming = false;
                    
                    cursor.remove();
                    
                    out.appendChild(document.createElement("br"));
                    out.appendChild(document.createElement("br"));
                    
                    document.getElementById("inputBox").disabled = false;
                    document.querySelector('button').disabled = false;
                    document.getElementById("inputBox").focus();
                    
                    // æœ€ç»ˆæ»šåŠ¨åˆ°åº•éƒ¨
                    out.scrollTop = out.scrollHeight;
                }
            }, 30);
        }
      
      function showStatus(message) {
          // Remove any existing status
          hideStatus();
          
          const out = document.getElementById("outputBox");
          const statusElement = document.createElement("div");
          statusElement.id = "status-indicator";
          statusElement.className = "status-indicator";
          statusElement.innerHTML = message;
          
          out.appendChild(statusElement);
          out.scrollTop = out.scrollHeight;
      }
      
      function hideStatus() {
          const statusElement = document.getElementById("status-indicator");
          if (statusElement) {
              statusElement.remove();
          }
      }

    </script>
</body>
</html>